---
date: 2025-10-10
author:
  - Siyuan Liu
tags:
  - Huggingface
---
# AutoModel
```
from transformers import AutoModel

model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```
transformeråº“ä¸­å·²ç»é¢„å…ˆæ³¨å†Œäº†ä¸€äº›ç»å…¸æ¨¡å‹çš„è®°å½•ï¼Œè¿™äº›è®°å½•å°±æ˜¯ä¸€ä¸ª`AutoModel`ï¼Œè°ƒç”¨`from_pretrained`å°±ä¼šè‡ªåŠ¨æ ¹æ®æ¨¡å‹åç§°å»æ‹‰å–æœåŠ¡å™¨ä¸Šè¿™ä¸ªæ¨¡å‹çš„`config file`å’Œ`model file`ï¼Œç„¶åè‡ªåŠ¨å®ä¾‹åŒ–æ¨¡å‹

---
## AutoModelForSequenceClassification
å¯¹äºæ–‡æœ¬ï¼ˆæˆ–åºåˆ—ï¼‰åˆ†ç±»ï¼Œä½ åº”è¯¥åŠ è½½`AutoModelForSequenceClassification`

```
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

---
## å¦‚æœæƒ³å°†è‡ªå®šä¹‰çš„æ¨¡å‹æ•°æ®ï¼Œæ³¨å†Œè¿›AutoModelä¸­
```
from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)
```
é¦–å…ˆéœ€è¦ç»™è‡ªå®šä¹‰çš„æ¨¡å‹åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼Œæ¯”å¦‚`"new-model"`ï¼ˆè¯¦ç»†è¯¥å¦‚ä½•ç¼–å†™å¯ä»¥çœ‹åé¢ï¼‰ï¼Œå°†è¿™ä¸ªè‡ªå®šä¹‰çš„æ¨¡å‹çš„é…ç½®æ–‡ä»¶ç±»å‘½åä¸º`NewModelConfig`ã€‚ç„¶åå°†`NewModelConfig`ä¼ é€’ç»™`.register()`æ–¹æ³•ï¼Œå‘½åä¸º`NewModel`ã€‚ä¹‹åå°±å¯ä»¥é€šè¿‡`AutoModel.from_pretrained("NewModel")`å®ä¾‹åŒ–æ¨¡å‹äº†

---
# .from_pretrained()
## å‚æ•°
- **pretrained_model_name_or_path**Â (`str`Â orÂ `os.PathLike`) â€” Can be either:
    
    - A string, theÂ _model id_Â of a pretrained model configuration hosted inside a model repo on huggingface.co.
    - A path to aÂ _directory_Â containing a configuration file saved using theÂ [save_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PreTrainedConfig.save_pretrained)Â method, or theÂ [save_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)Â method, e.g.,Â `./my_model_directory/`.
    - A path or url to a saved configuration JSONÂ _file_, e.g.,Â `./my_model_directory/configuration.json`.
    
- [](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained.cache_dir)**cache_dir**Â (`str`Â orÂ `os.PathLike`,Â _optional_) â€” Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.
- [](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained.force_download)**force_download**Â (`bool`,Â _optional_, defaults toÂ `False`) â€” Whether or not to force the (re-)download the model weights and configuration files and override the cached versions if they exist.
- [](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained.proxies)**proxies**Â (`dict[str, str]`,Â _optional_) â€” A dictionary of proxy servers to use by protocol or endpoint, e.g.,Â `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
- [](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained.revision)**revision**Â (`str`,Â _optional_, defaults toÂ `"main"`) â€” The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co, soÂ `revision`Â can be any identifier allowed by git.
- [](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained.return_unused_kwargs)**return_unused_kwargs**Â (`bool`,Â _optional_, defaults toÂ `False`) â€” IfÂ `False`, then this function returns just the final configuration object.
    
    IfÂ `True`, then this functions returns aÂ `Tuple(config, unused_kwargs)`Â whereÂ _unused_kwargs_Â is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the part ofÂ `kwargs`Â which has not been used to updateÂ `config`Â and is otherwise ignored.
    
- [](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained.trust_remote_code)**trust_remote_code**Â (`bool`,Â _optional_, defaults toÂ `False`) â€” Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set toÂ `True`Â for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.
- [](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained.kwargs\(additional)**kwargs(additional**Â keyword arguments,Â _optional_) â€” The values in kwargs of any keys which are configuration attributes will be used to override the loaded values. Behavior concerning key/value pairs whose keys areÂ _not_Â configuration attributes is controlled by theÂ `return_unused_kwargs`Â keyword parameter.

---
## è¿è¡Œæµç¨‹

![[Pasted image 20251010204149.png]]
1. å»æ‹‰å–æœåŠ¡å™¨ä¸Šè¿™ä¸ªæ¨¡å‹çš„`config file`å’Œ`model file`
2. æ‰“å¼€`config file`æ ¹æ®æ¨¡å‹ åå­—/åœ°å€ å¯»æ‰¾åº”è¯¥ä½¿ç”¨çš„`AutoConfig`(ä¾‹å¦‚GPT2ï¼ŒDeepseek-v1ï¼Œetc.)
3. å®ä¾‹åŒ–`AutoConfig`ç±»ã€‚æ‰¾åˆ°å¯¹åº”çš„`model class`
4. æ ¹æ®`AutoConfig`å’Œ`model class`ï¼Œå®ä¾‹åŒ–ä¸€ä¸ªå®Œæ•´çš„`model`(è¿˜æ˜¯éšæœºçš„weits)
5. ä»`model file`ä¸­åŠ è½½æƒé‡åˆ°`model`ä¸­

---
# AutoConfig
## ç¤ºä¾‹
```
class BertConfig(PreTrainedConfig):
model_type = "bert"  
  
    def __init__(  
        self,  
        vocab_size=30522,  
        hidden_size=768,  
        num_hidden_layers=12,  
        num_attention_heads=12,  
        intermediate_size=3072,  
        hidden_act="gelu",  
        hidden_dropout_prob=0.1,  
        attention_probs_dropout_prob=0.1,  
        max_position_embeddings=512,  
        type_vocab_size=2,  
        initializer_range=0.02,  
        layer_norm_eps=1e-12,  
        pad_token_id=0,  
        use_cache=True,  
        classifier_dropout=None,  
        **kwargs,  
    ):  
        super().__init__(pad_token_id=pad_token_id, **kwargs)  
  
        self.vocab_size = vocab_size  
        self.hidden_size = hidden_size  
        self.num_hidden_layers = num_hidden_layers  
        self.num_attention_heads = num_attention_heads  
        self.hidden_act = hidden_act  
        self.intermediate_size = intermediate_size  
        self.hidden_dropout_prob = hidden_dropout_prob  
        self.attention_probs_dropout_prob = attention_probs_dropout_prob  
        self.max_position_embeddings = max_position_embeddings  
        self.type_vocab_size = type_vocab_size  
        self.initializer_range = initializer_range  
        self.layer_norm_eps = layer_norm_eps  
        self.use_cache = use_cache  
        self.classifier_dropout = classifier_dropout  
  
  
class BertOnnxConfig(OnnxConfig):  
    @property  
    def inputs(self) -> Mapping[str, Mapping[int, str]]:  
        if self.task == "multiple-choice":  
            dynamic_axis = {0: "batch", 1: "choice", 2: "sequence"}  
        else:  
            dynamic_axis = {0: "batch", 1: "sequence"}  
        return OrderedDict(  
            [  
                ("input_ids", dynamic_axis),  
                ("attention_mask", dynamic_axis),  
                ("token_type_ids", dynamic_axis),  
            ]  
        )
```

åˆ›å»ºæŒ‡å®š`model`çš„`AutoConfig`ç±»
```
from transformers import BertConfig

bert_config = BertConfig.from_pretrained("bert-base-cased")
```
## è‡ªå®šä¹‰æ¨¡å‹æ„å»º
ä½ å¯ä»¥ä¿®æ”¹æ¨¡å‹çš„é…ç½®ç±»æ¥æ”¹å˜æ¨¡å‹çš„æ„å»ºæ–¹å¼ã€‚é…ç½®æŒ‡æ˜äº†æ¨¡å‹çš„å±æ€§ï¼Œæ¯”å¦‚éšè—å±‚æˆ–è€…æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚

```
from transformers import AutoConfig

my_config = AutoConfig.from_pretrained("distilbert/distilbert-base-uncased", n_heads=12)
```

---
# ä¿å­˜/åŠ è½½ å½“å‰æ¨¡å‹æƒé‡
```
from transformers import BertModel, BertConfig

bert_config = BertConfig.from_pretrained("google-bert/bert-base-cased")
bert_model = BertModel(bert_config)
.
.
.

bert_model.save_pretrained("my_bert_model") # åœ¨ my_bert_model ç›®å½•ä¸‹ä¿å­˜
```

```
from transformers import BertModel

bert_model = BertModel.from_pretrained("my_bert_model") 
```

---
# AutoTokenizer
åˆ†è¯å™¨è´Ÿè´£é¢„å¤„ç†æ–‡æœ¬ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºç”¨äºè¾“å…¥æ¨¡å‹çš„æ•°å­—æ•°ç»„ã€‚æœ‰å¤šä¸ªç”¨æ¥ç®¡ç†åˆ†è¯è¿‡ç¨‹çš„è§„åˆ™ï¼ŒåŒ…æ‹¬å¦‚ä½•æ‹†åˆ†å•è¯å’Œåœ¨ä»€ä¹ˆæ ·çš„çº§åˆ«ä¸Šæ‹†åˆ†å•è¯ï¼ˆåœ¨Â [åˆ†è¯å™¨æ€»ç»“](https://huggingface.co/docs/transformers/zh/tokenizer_summary)Â å­¦ä¹ æ›´å¤šå…³äºåˆ†è¯çš„ä¿¡æ¯ï¼‰ã€‚è¦è®°ä½æœ€é‡è¦çš„æ˜¯å®ä¾‹åŒ–çš„åˆ†è¯å™¨åç§°è¦ä¸æ¨¡å‹çš„åç§°ç›¸åŒ, æ¥ç¡®ä¿å’Œæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨ç›¸åŒçš„åˆ†è¯è§„åˆ™ã€‚

å®ä¾‹åŒ–`AutoTokenizer`
```
from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

**å°†æ–‡æœ¬ä¼ å…¥åˆ†è¯å™¨**
```
encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(encoding)
```
åˆ†è¯å™¨è¿”å›äº†å«æœ‰å¦‚ä¸‹å†…å®¹çš„å­—å…¸:
- [input_ids](https://huggingface.co/docs/transformers/zh/glossary#input-ids)ï¼šç”¨æ•°å­—è¡¨ç¤ºçš„ token
- [attention_mask](https://huggingface.co/docs/transformers/zh/.glossary#attention-mask)ï¼šåº”è¯¥å…³æ³¨å“ªäº› token çš„æŒ‡ç¤º

**åˆ†è¯å™¨ä¹Ÿå¯ä»¥æ¥å—åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶å¡«å……å’Œæˆªæ–­æ–‡æœ¬ï¼Œè¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹æ¬¡**
```
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
```

# Trainer ä¼˜åŒ–è®­ç»ƒå¾ªç¯
è¿™ä¸ªç±»åªé€‚ç”¨äºhuggingfaceçš„`PreTrainedModel` å’Œ `torch.nn.Module`

```
from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer
from datasets import load_dataset

model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT

# åˆ›å»ºä¸€ä¸ªç»™æ•°æ®é›†åˆ†è¯çš„å‡½æ•°ï¼Œå¹¶ä¸”ä½¿ç”¨Â `map`Â åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†
def tokenize_dataset(dataset):
# åªå¯¹"text"åˆ—çš„å†…å®¹è¿›è¡Œåˆ†ç‰‡
    return tokenizer(dataset["text"])

# batched=True è¡¨ç¤ºæŒ‰æ‰¹å¤„ç†ï¼ˆé»˜è®¤æ‰¹å¤§å°çº¦ 1000ï¼Œå¯ç”¨ batch_size=â€¦ è°ƒæ•´ï¼‰
dataset = dataset.map(tokenize_dataset, batched=True)

# ç”¨æ¥ä»æ•°æ®é›†ä¸­åˆ›å»ºæ‰¹æ¬¡çš„Â [DataCollatorWithPadding]
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
)  # doctest: +SKIP

# å¯¹äºåƒç¿»è¯‘æˆ–æ‘˜è¦è¿™äº›ä½¿ç”¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ä»»åŠ¡,ç”¨Â `Seq2SeqTrainer`Â å’ŒÂ `Seq2SeqTrainingArguments`Â æ¥æ›¿ä»£
trainer.train()
```