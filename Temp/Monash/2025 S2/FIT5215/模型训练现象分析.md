---
date: 2025-08-07
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - summary
---
# 过拟合
![[Pasted image 20250807171140.png]]
**表现:**
- **Loss 曲线:**
    - 训练集 Loss 快速下降，loss 值很低
    - 验证集 Loss 先下降后上升，在后期高于训练集 Loss
    - 训练集和验证集 Loss 曲线分离
- **Accuracy 曲线:**
    - 训练集 Accuracy 快速提升，accuracy 值很高
    - 验证集 Accuracy 先提升后下降，在后期低于训练集 Accuracy
    - 训练集和验证集 Accuracy 曲线分离
**原因:**
1. **训练数据量不足**
    - 当训练数据集的规模太小，模型很容易记住训练数据中的噪声和细微的变化，而不是学习到数据背后更泛化的规律
    - 模型在训练集上表现很好，但在未见过的新数据上表现很差

2. **模型复杂度过高**
    - 模型过于复杂，例如层数过多、神经元数量过多，或使用了过于复杂的网络结构，导致模型具有过强的学习能力
    - 模型能够拟合训练数据中的所有细节，包括噪声，从而失去泛化能力

3. **训练时间过长**
    - 训练时间过长会导致模型过度优化，逐渐记住训练数据中的噪声和特例，而不是学习到数据的本质特征
    - 模型在训练初期可能具有较好的泛化能力，但随着训练的进行，泛化能力逐渐下降
- 
1. **特征选择不当**
    - 选择了不相关的或冗余的特征，这些特征会引入噪声，影响模型的泛化能力
    - 特征的维度过高，导致模型需要学习大量的参数，容易发生过拟合

2. **噪声数据**
    - 训练数据中存在大量的错误标签、异常值或不一致的数据，这些噪声会干扰模型的学习过程
    - 模型会将噪声数据误认为真实数据，从而导致过拟合

3. **正则化不足**
    - 没有使用或使用了较弱的正则化方法，导致模型没有受到足够的约束，可以自由地拟合训练数据中的噪声
    - 常见的正则化方法包括L1正则化、L2正则化、Dropout等

4. **早停策略不当**
    - 没有使用早停策略，或者早停策略不够敏感，导致模型在泛化能力开始下降后仍然继续训练
    - 早停策略应该在验证集上的性能开始下降时停止训练

**解决方法:**
1. **增加训练数据**
    - 数据增强 (Data Augmentation): 通过对现有训练数据进行各种变换（例如旋转、翻转、缩放、裁剪、平移、添加噪声等）来生成新的训练样本，从而扩大数据集的规模
    - 收集更多真实数据: 如果条件允许，尽量收集更多真实的数据，这是解决过拟合最有效的方法之一

2. **简化模型**
    - 减少模型层数: 减少神经网络的层数，降低模型的深度
    - 减少神经元数量: 减少每层神经网络的神经元数量，降低模型的宽度
    - 参数共享: 在某些网络结构中，可以采用参数共享的方式来减少模型的参数量
    - 选择更简单的模型结构: 例如，可以考虑使用线性模型、决策树等更简单的模型，而不是复杂的神经网络

3. **正则化**
    - L1 正则化 (Lasso): 在损失函数中添加 L1 正则项，鼓励模型学习稀疏的权重，即让更多的权重变为 0，从而减少模型的复杂度
    - L2 正则化 (Ridge): 在损失函数中添加 L2 正则项，限制权重的平方和，使得权重更加平滑，从而提高模型的泛化能力
    - Dropout: 在训练过程中，随机地丢弃一部分神经元，强制模型学习更加鲁棒的特征，避免模型过度依赖某些特定的神经元
    - Batch Normalization: 在每层神经网络的输入之前进行归一化，使得输入数据的分布更加稳定，从而加速训练并提高模型的泛化能力

4. **早停策略 (Early Stopping)**
    - 监控验证集性能: 在训练过程中，定期评估模型在验证集上的性能
    - 提前停止训练: 当验证集上的性能开始下降时，停止训练，避免模型继续学习训练数据中的噪声

5. **特征选择**
    - 选择相关特征: 选择与目标变量相关的特征，去除不相关的或冗余的特征
    - 特征降维: 使用降维技术（例如主成分分析 PCA）来降低特征的维度，减少模型的参数量

6. **数据清洗**
    - 去除噪声数据: 检查训练数据，去除错误标签、异常值或不一致的数据
    - 数据平滑: 对数据进行平滑处理，减少噪声的影响

7. **集成学习**
    - Bagging: 通过对训练数据进行多次采样，训练多个模型，然后对它们的预测结果进行平均或投票，从而提高模型的泛化能力
    - Boosting: 通过迭代地训练多个模型，每个模型都关注之前模型预测错误的样本，从而提高模型的准确率和泛化能力
# 欠拟合
![[Pasted image 20250807174743.png]]
**表现:**
- **Loss 曲线:**
    - 训练集 Loss 较高，且下降缓慢
    - 验证集 Loss 较高，且下降缓慢，与训练集 Loss 曲线接近
    - 两条曲线在整个训练过程中都保持较高的 Loss 值
- **Accuracy 曲线:**
    - 训练集 Accuracy 较低，且提升缓慢
    - 验证集 Accuracy 较低，且提升缓慢，与训练集 Accuracy 曲线接近
    - 两条曲线在整个训练过程中都保持较低的 Accuracy 值

**原因:**
1. **模型复杂度不足**
    - 模型过于简单，例如使用了线性模型来拟合非线性数据，或者神经网络的层数和神经元数量不足，导致模型无法捕捉到数据中的复杂关系

2. **特征工程不足**
    - 选取的特征无法充分表达数据的特点，或者特征的质量不高，例如存在大量的缺失值、噪声或不相关特征

3. **训练时间不足**
    - 模型训练的时间不够长，导致模型还没有充分学习到数据中的模式

4. **正则化过度**
    - 使用了过强的正则化方法（例如L1或L2正则化），导致模型受到过多的约束，无法充分拟合训练数据

5. **学习率过高或过低**
    - 学习率过高会导致模型在训练过程中震荡，无法收敛到最优解
    - 学习率过低会导致模型收敛速度过慢，甚至无法收敛

6. **数据预处理不当**
    - 没有对数据进行归一化或标准化，导致模型的训练效率降低
    - 数据存在偏差或不平衡，导致模型学习到的模式不准确

7. **优化算法选择不当**
    - 选择的优化算法不适合当前的模型和数据，导致模型无法有效地学习

**解决方法:**
1. **增加模型复杂度**
- 增加网络层数 (Deeper Network): 增加神经网络的层数，使模型能够学习更复杂的特征表示
- 增加每层神经元数量 (Wider Network): 增加每层网络的神经元数量，提高模型的容量
- 使用更复杂的模型结构: 例如，将线性模型替换为非线性模型，或者使用更高级的神经网络结构，如残差网络 (ResNet)、Transformer 等

1. **改进特征工程**
- 增加相关特征: 挖掘更多与目标变量相关的特征，提供更丰富的信息
- 特征组合: 将多个特征进行组合，生成新的特征，例如将两个特征相乘或相加
- 特征变换: 对特征进行变换，例如进行多项式扩展、对数变换、指数变换等，使特征更适合模型的学习
- 处理缺失值: 使用合适的方法填充缺失值，例如使用均值、中位数或众数填充，或者使用模型预测缺失值
- 处理异常值: 检测并处理异常值，例如将异常值替换为合理的值，或者将异常值移除

1. **延长训练时间**
- 增加训练轮数 (Epochs): 增加训练轮数，使模型能够充分学习到数据中的模式
- 监控训练过程: 监控训练集和验证集的损失函数值，确保模型在训练过程中持续学习

1. **减少正则化**
- 减小 L1/L2 正则化系数: 减小 L1 或 L2 正则化系数，降低模型受到的约束，使其能够更好地拟合训练数据
- 减少 Dropout 比例: 减少 Dropout 比例，使更多的神经元参与训练，提高模型的容量

1. **调整学习率**
- 使用更小的学习率: 如果学习率过高导致模型无法收敛，可以尝试使用更小的学习率
- 使用学习率衰减策略: 在训练过程中逐渐降低学习率，例如使用 step decay、exponential decay 或 cosine annealing 等策略
- 自适应学习率算法: 使用自适应学习率算法，例如 Adam、RMSprop 或 Adagrad 等，这些算法可以自动调整学习率

1. **改进数据预处理**
- 数据归一化/标准化: 对数据进行归一化或标准化，使数据的分布更加均匀，提高模型的训练效率
- 处理数据偏差: 如果数据存在偏差，可以尝试使用重采样或重加权等方法来平衡数据

1. **优化算法选择**
- 尝试不同的优化算法: 选择适合当前模型和数据的优化算法，例如 Adam、SGD 或 RMSprop 等

1. **使用集成学习**
- Bagging: 使用 Bagging 方法训练多个模型，然后对它们的预测结果进行平均或投票，从而提高模型的泛化能力
- Boosting: 使用 Boosting 方法训练多个模型，每个模型都关注之前模型预测错误的样本，从而提高模型的准确率和泛化能力

# 梯度消失
==常见于sigmoid / tanh激活函数==
![[Pasted image 20250807174530.png]]
**表现:**
- **Loss 曲线:**
    - 训练集 Loss 下降缓慢
    - 训练集 Loss 在后期几乎不再下降，趋于平缓
    - 验证集 Loss 也呈现相似的趋势
- **Accuracy 曲线:**
    - 训练集 Accuracy 提升缓慢
    - 训练集 Accuracy 在后期几乎不再提升，趋于平缓
    - 验证集 Accuracy 也呈现相似的趋势

**原因:**
1. **激活函数的选择：**
    - **Sigmoid 和 Tanh:** 这些激活函数在输入值很大或很小时，梯度会变得非常小（接近于 0）。 这是因为它们的导数在这些区域接近于 0。 当网络很深时，这些小的梯度在反向传播过程中累积相乘，导致梯度逐渐消失。
2. **网络深度：**
    - **深层网络：** 梯度消失问题在深层网络中更为常见。 这是因为梯度需要通过多个层反向传播，每经过一层，梯度都会衰减一些。 当网络很深时，这种衰减会变得非常严重，导致前面的层几乎无法学习。
3. **权重初始化不当：**
    - **过大或过小的初始权重：** 如果权重初始化过大，可能会导致激活函数输出饱和值，从而导致梯度消失。 如果权重初始化过小，梯度在反向传播过程中会逐渐缩小，最终消失。
4. **学习率过大：**
    - **不稳定的训练：** 学习率过大可能会导致训练过程不稳定，使得模型在损失函数的表面上跳来跳去，难以收敛到一个好的解。 在这种情况下，梯度可能会变得非常小，导致训练停滞。
5. **优化算法：**
    - **传统梯度下降：** 传统的梯度下降算法可能难以处理梯度消失问题。
6. **数据预处理不当：**
    - **未归一化/标准化：** 如果输入数据没有进行归一化或标准化，可能会导致某些层的输入值很大或很小，从而导致梯度消失。

**解决方法:**
1. **选择合适的激活函数：**
    
    - **ReLU (Rectified Linear Unit):** ReLU 激活函数在输入大于 0 时，梯度为 1，可以有效地缓解梯度消失问题。
    - **Leaky ReLU:** Leaky ReLU 在输入小于 0 时，梯度为一个较小的常数（例如 0.01），可以避免 ReLU 的 "dying ReLU" 问题。
    - **ELU (Exponential Linear Unit):** ELU 结合了 ReLU 和 Leaky ReLU 的优点，在输入小于 0 时，输出为一个负值，可以避免 "dying ReLU" 问题，并且具有更好的鲁棒性。
    - **SELU (Scaled Exponential Linear Unit):** SELU 是一种自归一化的激活函数，可以使网络的输出保持在一个稳定的范围内，从而避免梯度消失和梯度爆炸问题。
2. **使用 Batch Normalization：**
    
    - Batch Normalization 可以对每一层的输入进行归一化，使得输入数据的分布更加稳定，从而加速训练并提高模型的泛化能力。
    - Batch Normalization 还可以缓解梯度消失和梯度爆炸问题。
3. **使用残差连接 (Residual Connections)：**
    
    - 残差连接可以将浅层的梯度直接传递到深层，避免梯度在反向传播过程中逐渐衰减。
    - 残差连接是 ResNet 等深层网络能够成功训练的关键。
4. **合理的权重初始化：**
    
    - **Xavier 初始化：** Xavier 初始化是一种根据输入和输出神经元的数量来确定权重初始值的策略，可以使得每一层的输入和输出具有相似的方差，从而避免梯度消失和梯度爆炸问题。
    - **He 初始化：** He 初始化是 ReLU 激活函数的推荐初始化方法，可以更好地适应 ReLU 的特性。
5. **调整学习率：**
    
    - **使用较小的学习率：** 较小的学习率可以使得训练过程更加稳定，避免梯度消失。
    - **使用学习率衰减策略：** 在训练过程中逐渐降低学习率，可以使得模型在训练初期快速收敛，在训练后期更加稳定。
    - **自适应学习率算法：** 使用自适应学习率算法，例如 Adam、RMSprop 或 Adagrad 等，这些算法可以自动调整学习率，从而避免梯度消失。
6. **使用 LSTM 或 GRU：**
    
    - LSTM (Long Short-Term Memory) 和 GRU (Gated Recurrent Unit) 是一种特殊的循环神经网络，具有记忆单元和门控机制，可以有效地缓解梯度消失问题，从而能够处理长序列数据。
7. **梯度裁剪 (Gradient Clipping)：**
    
    - 梯度裁剪可以限制梯度的最大值，避免梯度爆炸，从而间接地缓解梯度消失问题。
8. **使用更深的网络结构：**
    
    - 虽然梯度消失问题在深层网络中更为常见，但使用更深的网络结构可以提高模型的表达能力。 可以结合使用残差连接、Batch Normalization 等方法来缓解梯度消失问题。
# 梯度爆炸
==常见于sigmoid / tanh激活函数==
![[Pasted image 20250807173729.png]]
**表现:**
- **Loss 曲线:**
    - 训练集 Loss 在初期迅速增加
    - 训练集 Loss 出现明显的震荡，可能包含 NaN 值
    - 验证集 Loss 也可能受到影响，出现震荡
- **Accuracy 曲线:**
    - 训练集 Accuracy 在初期迅速下降
    - 训练集 Accuracy 出现明显的震荡
    - 验证集 Accuracy 也可能受到影响，出现下降
    - 
**原因:**
1. **激活函数的选择：**
    - **某些激活函数在特定区域梯度过大:** 某些激活函数，例如 ReLU，在输入值很大时，梯度为 1。如果网络中的权重过大，经过 ReLU 后，梯度可能会变得非常大，导致梯度爆炸。
2. **权重初始化不当：**
    - **过大的初始权重：** 如果权重初始化过大，可能会导致激活函数的输出值很大，从而导致梯度爆炸。
3. **网络深度：**
    - **深层网络：** 在深层网络中，梯度需要通过多个层反向传播。 如果每一层的梯度都大于 1，那么经过多层累积，梯度可能会变得非常大，导致梯度爆炸。
4. **学习率过大：**
    - **不稳定的训练：** 学习率过大可能会导致训练过程不稳定，使得模型在损失函数的表面上跳来跳去。 在这种情况下，梯度可能会变得非常大，导致梯度爆炸。
5. **数据预处理不当：**
    - **未归一化/标准化：** 如果输入数据没有进行归一化或标准化，可能会导致某些层的输入值很大，从而导致梯度爆炸。
6. **循环神经网络 (RNN) 的长期依赖：**
    - **梯度累积：** 在 RNN 中，梯度需要通过时间步反向传播。 如果序列很长，梯度可能会在时间步上累积，导致梯度爆炸。

**解决方法:**
1. **梯度裁剪 (Gradient Clipping)：**
    
    - **限制梯度大小：** 梯度裁剪是一种常用的解决梯度爆炸问题的方法。 它通过设置一个梯度阈值，当梯度超过这个阈值时，将其缩放到阈值大小。 这可以有效地防止梯度变得过大，从而避免梯度爆炸。
    - **类型：**
        - **值裁剪 (Clipping by Value)：** 直接限制梯度的数值范围。
        - **范数裁剪 (Clipping by Norm)：** 限制梯度的范数（例如 L2 范数）。
2. **正则化：**
    
    - **L1/L2 正则化：** 通过在损失函数中添加 L1 或 L2 正则化项，可以限制权重的增长，从而避免梯度爆炸。
3. **Batch Normalization：**
    
    - **稳定数据分布：** Batch Normalization 可以对每一层的输入进行归一化，使得输入数据的分布更加稳定，从而加速训练并提高模型的泛化能力。
    - **限制梯度：** Batch Normalization 还可以限制梯度的大小，从而缓解梯度爆炸问题。
4. **使用 ReLU 及其变体：**
    
    - **避免饱和：** ReLU 及其变体（例如 Leaky ReLU、ELU）可以避免 Sigmoid 和 Tanh 等激活函数在输入值很大或很小时出现梯度饱和的情况，从而减少梯度爆炸的可能性。
5. **调整学习率：**
    
    - **减小学习率：** 较小的学习率可以使得训练过程更加稳定，避免梯度爆炸。
    - **自适应学习率算法：** 使用自适应学习率算法，例如 Adam、RMSprop 或 Adagrad 等，这些算法可以自动调整学习率，从而避免梯度爆炸。
6. **权重初始化：**
    
    - **合理的初始化策略：** 使用合理的权重初始化策略（例如 Xavier 初始化、He 初始化）可以使得每一层的输入和输出具有相似的方差，从而避免梯度爆炸。
7. **梯度 Masking：**
    
    - **选择性更新：** 在某些情况下，只有一部分参数的梯度会爆炸。 可以使用梯度 Masking 方法，只更新梯度正常的参数，而忽略梯度爆炸的参数。
8. **重新设计网络结构：**
    - **减少网络深度：** 在某些情况下，梯度爆炸可能是由于网络过深导致的。 可以尝试减少网络的深度，或者使用更有效的网络结构（例如 ResNet）来缓解梯度爆炸问题

9. **梯度截断 (Truncated Backpropagation Through Time, TBPTT)** ==针对 RNN 的特殊方法==
    -  在 RNN 中，可以限制反向传播的时间步数，从而减少梯度爆炸的可能性。
# 损失函数震荡
![[Pasted image 20250807175412.png]]
**原因:**
1. **学习率过大：**
    - **跳跃性更新：** 学习率决定了每次迭代中参数更新的幅度。 如果学习率过大，模型可能会在损失函数的表面上跳跃，难以收敛到最优解，导致损失函数值震荡。
2. **Batch Size 过小：**
    - **梯度估计不稳定：** Batch Size 决定了每次迭代中使用的样本数量。 如果 Batch Size 过小，每次迭代计算出的梯度可能无法准确地代表整个数据集的梯度，导致梯度估计不稳定，从而导致损失函数值震荡。
3. **数据集存在噪声：**
    - **异常样本干扰：** 如果数据集中存在噪声（例如异常值、错误标签），模型在训练过程中可能会受到这些噪声的影响，导致损失函数值震荡。
4. **模型复杂度过高：**
    - **过度拟合：** 如果模型过于复杂，可能会过度拟合训练数据，导致模型在训练集上表现很好，但在验证集上表现很差。 在这种情况下，损失函数值可能会在训练过程中震荡。
5. **优化算法选择不当：**
    - **不合适的算法：** 某些优化算法（例如传统的梯度下降算法）可能难以处理损失函数表面上的局部最小值或鞍点，导致损失函数值震荡。
6. **梯度爆炸：**
    - **数值不稳定：** 梯度爆炸会导致梯度值变得非常大，从而导致损失函数值震荡。
7. **激活函数选择不当：**
    - **梯度消失/爆炸：** 某些激活函数（例如 Sigmoid 和 Tanh）在输入值很大或很小时，梯度会变得非常小或非常大，从而导致损失函数值震荡。
8. **数据预处理不当：**
    - **未归一化/标准化：** 如果输入数据没有进行归一化或标准化，可能会导致某些层的输入值很大，从而导致损失函数值震荡。
9. **训练数据和验证数据分布不一致：**
    - **泛化能力差：** 如果训练数据和验证数据的分布不一致，模型在训练集上学习到的模式可能无法很好地泛化到验证集上，从而导致损失函数值震荡。

**解决方法:**
1. **调整学习率：**
    
    - **减小学习率：** 这是最常见的解决方法。 尝试使用更小的学习率，例如将学习率降低 10 倍。
    - **学习率衰减：** 使用学习率衰减策略，例如 step decay、exponential decay 或 cosine annealing 等，在训练过程中逐渐降低学习率。 这可以使得模型在训练初期快速收敛，在训练后期更加稳定。
    - **自适应学习率算法：** 使用自适应学习率算法，例如 Adam、RMSprop 或 Adagrad 等，这些算法可以自动调整学习率，从而避免损失函数值震荡。
2. **调整 Batch Size：**
    
    - **增大 Batch Size：** 增大 Batch Size 可以使得每次迭代计算出的梯度更加准确地代表整个数据集的梯度，从而减少梯度估计的方差，降低损失函数值的震荡。 但过大的 Batch Size 可能会导致内存不足，需要根据实际情况进行调整。
3. **数据清洗和预处理：**
    
    - **去除噪声数据：** 检查数据集，去除异常值、错误标签等噪声数据。
    - **数据归一化/标准化：** 对数据进行归一化或标准化，使得数据的分布更加均匀，从而提高模型的训练效率和稳定性。
4. **简化模型：**
    
    - **减少模型复杂度：** 如果模型过于复杂，可以尝试减少模型的层数或参数数量，降低模型的过拟合风险。
    - **使用更简单的模型结构：** 例如，将复杂的神经网络替换为更简单的线性模型或决策树模型。
5. **选择合适的优化算法：**
    
    - **尝试不同的优化算法：** 选择适合当前模型和数据的优化算法，例如 Adam、RMSprop 或 SGD 等。 不同的优化算法具有不同的特性，可以根据实际情况进行选择。

6. **使用合适的激活函数：**
    
    - **避免梯度消失/爆炸：** 选择合适的激活函数，例如 ReLU 及其变体，可以避免梯度消失和梯度爆炸问题，从而提高模型的训练稳定性。

7. **集成学习：**
    
    - **模型平均：** 使用集成学习方法，例如 Bagging 或 Boosting，训练多个模型，然后对它们的预测结果进行平均或投票，从而提高模型的泛化能力和鲁棒性。