---
date: 2025-11-09
author:
  - Siyuan Liu
tags:
  - FIT5215
---
# Diffusion
## Forward Diffusion Process
Formulated as Markov chain with ğ‘‡ steps (usually large)

**At time 0:**
- Sample data point $ğ‘¥_0$ from real data distribution $ğ‘(ğ‘¥)$ (i.e., from training data)

**At time t:**
- $x_t= x_{t-1}+ GaussianNoise$
- $ğ‘¥_ğ‘¡$ is now distributed according to $q(x_t|x_{t-1})=N(Â·|\mu_t,\epsilon_t)$ 
	- $\mu_t=\sqrt{1-\beta_t}\times x_{t-1}$
	- $Î£_ğ‘¡= ğ›½_ğ‘¡ ğ‘°$
	- $0 < ğ›½_ğ‘¡< 1$

### How to sample $ğ‘¥_ğ‘¡$ from $N(x_t;\sqrt{1-\beta_t}\times x_{t-1},ğ›½_ğ‘¡ ğ‘°)$
use [[Reparameterization Trick]] to get 
$$x_t= \sqrt{1-\beta_t}\times x_{t-1}+\sqrt{\beta_t}\times \epsilon_{t-1}$$
### Is there an analytical form for $ğ‘¥_ğ‘¡$ directly from $ğ‘¥_0$
![[Pasted image 20251109154007.png]]
![[Pasted image 20251109154038.png]]
## Backward Diffusion Process
Iteratively denoise from time step T to 0

**At time T:**
- Sample a random vector $ğ‘¥_ğ‘‡$ from $\mathbf{N}(0,\mathbf{I})$

**At time t:**
- $x_{t-1}=x_t - \text{amount of noise}$
- $x_{t-1}$ is now distributed according to $q(x_{t-1}|x_t)$
![[Pasted image 20251109154434.png]]
### U-net
åœ¨æ‰©æ•£æ¨¡å‹çš„**åå‘è¿‡ç¨‹ (Reverse Process)** ä¸­ï¼ŒU-Net é€šè¿‡é¢„æµ‹åŸå§‹æ·»åŠ çš„å™ªå£°ï¼Œæ¥è¾¾åˆ°å»å™ªçš„ç›®æ ‡

U-Net æ¨¡å‹å¯¹å™ªå£°çš„**é¢„æµ‹å€¼**æ˜¯$Ïµ_Î¸(xâ‚œ, t)$ï¼š
- **è¾“å…¥**ï¼šæ¥æ”¶ä¸€ä¸ªåœ¨æ—¶é—´æ­¥ `t` çš„**å™ªå£°å›¾åƒ `xâ‚œ`** å’Œ**å½“å‰çš„æ—¶é—´æ­¥ `t`** æœ¬èº«
- **ç›®æ ‡**ï¼š**é¢„æµ‹**å‡ºå½“åˆä¸ºäº†ä»æ¸…æ™°å›¾åƒ `xâ‚€` ç”Ÿæˆ `xâ‚œ` æ—¶ï¼Œæ‰€åŠ å…¥çš„é‚£ä¸ª**åŸå§‹é«˜æ–¯å™ªå£° `Ïµ`**
- **`Î¸`**: ä»£è¡¨äº† U-Net ç½‘ç»œè‡ªèº«çš„å¯å­¦ä¹ å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰

## How to train?
![[Pasted image 20251109155759.png]]
- **Fix a forward process (å›ºå®šå‰å‘è¿‡ç¨‹)**:
    - å®šä¹‰äº†å‰å‘åŠ å™ªè¿‡ç¨‹ã€‚$x_t$ æ˜¯åœ¨ $t$ æ—¶åˆ»çš„åŠ å™ªå›¾åƒ
    - å…¬å¼ $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$ å±•ç¤ºäº†å¦‚ä½•ç›´æ¥ä»åŸå§‹å›¾åƒ $x_0$ é‡‡æ ·å¾—åˆ°ä»»æ„æ—¶åˆ» $t$ çš„å›¾åƒ $x_t$ã€‚è¿™é‡Œ $\epsilon$ æ˜¯æ ‡å‡†é«˜æ–¯å™ªå£° $\mathcal{N}(0, I)$
    - $\beta_t$ æ˜¯é¢„å®šä¹‰çš„å™ªå£°æ–¹å·®è°ƒåº¦ï¼Œ$\alpha_t = 1 - \beta_t$ï¼Œ$\bar{\alpha}_t$ æ˜¯ $\alpha_s$ çš„ç´¯ä¹˜

- **ç†æƒ³çš„åå‘è¿‡ç¨‹**:
    - æˆ‘ä»¬å¸Œæœ›ä»çº¯å™ªå£° $x_T \sim \mathcal{N}(0, I)$ å¼€å§‹ï¼Œé€šè¿‡ $q(x_{t-1}|x_t)$ ä¸€æ­¥æ­¥å»å™ªç”Ÿæˆæ–°æ ·æœ¬
    - **é—®é¢˜**: çœŸå®çš„åéªŒåˆ†å¸ƒ $q(x_{t-1}|x_t)$ æ˜¯ä¸å¯è®¡ç®—çš„ï¼ˆintractableï¼‰ï¼Œå› ä¸ºå®ƒéœ€è¦çŸ¥é“æ•´ä¸ªæ•°æ®åˆ†å¸ƒ

- **Solution (Ho et al.) (è§£å†³æ–¹æ¡ˆ)**:
    - **å…³é”®æ´å¯Ÿ**: å¦‚æœæˆ‘ä»¬çŸ¥é“åŸå§‹å›¾åƒ $x_0$ï¼Œé‚£ä¹ˆåéªŒåˆ†å¸ƒ $q(x_{t-1}|x_t, x_0)$ å°±å˜å¾—å¯è®¡ç®—äº†ï¼ˆtractableï¼‰ï¼Œå¹¶ä¸”å®ƒæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒ
    - ä½†æ˜¯ç”Ÿæˆæ—¶æˆ‘ä»¬æ²¡æœ‰ $x_0$
    - **æ–¹æ³•**: æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ $p_\theta(x_{t-1}|x_t)$ æ¥è¿‘ä¼¼è¿™ä¸ªçœŸå®çš„åéªŒåˆ†å¸ƒã€‚è¿™ç±»ä¼¼äºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œé€šè¿‡æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶çš„å˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰æ¥å®ç°ï¼Œè¿™ç­‰ä»·äºæœ€å°åŒ– $q(x_{t-1}|x_t, x_0)$ å’Œ $p_\theta(x_{t-1}|x_t)$ ä¹‹é—´çš„ KL æ•£åº¦

- **How to parameterise and learn $p_\theta(x_{t-1}|x_t)$? (å¦‚ä½•å‚æ•°åŒ–å’Œå­¦ä¹  $p_\theta$ï¼Ÿ)**
    - å‡è®¾ $p_\theta(x_{t-1}|x_t)$ ä¹Ÿæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ä¸º $\mu_\theta(x_t, t)$ï¼Œæ–¹å·® $\Sigma(x_t, t)$ è®¾ä¸ºå›ºå®šå€¼ $\sigma_t^2 I$ï¼ˆé€šå¸¸ $\sigma_t^2 = \beta_t$ æˆ– $\tilde{\beta}_t$ï¼‰
    - ç”±äºä¸¤ä¸ªåˆ†å¸ƒéƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒä¸”æ–¹å·®å›ºå®šç›¸åŒï¼Œæœ€å°åŒ–å®ƒä»¬çš„ KL æ•£åº¦å°±ç®€åŒ–ä¸ºæœ€å°åŒ–å®ƒä»¬å‡å€¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š$||\tilde{\mu}_t(x_t, x_0) - \mu_\theta(x_t, t)||^2$
    - æˆ‘ä»¬çŸ¥é“çœŸå®åéªŒå‡å€¼ $\tilde{\mu}_t$ çš„è§£æå½¢å¼ï¼ˆåŒ…å« $x_0$ å’Œå™ªå£° $\epsilon$ï¼‰
    - **æ ¸å¿ƒæ€æƒ³**: æˆ‘ä»¬ä¸ç›´æ¥é¢„æµ‹å‡å€¼ $\mu_\theta$ï¼Œè€Œæ˜¯é¢„æµ‹å™ªå£° $\epsilon$ã€‚æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªç½‘ç»œ $\epsilon_\theta(x_t, t)$ æ¥é¢„æµ‹æ·»åŠ åˆ°å›¾åƒä¸­çš„å™ªå£°
    - é€šè¿‡ä»£æ¢ï¼Œæœ€ç»ˆçš„æŸå¤±å‡½æ•° $L_{t-1}$ ç®€åŒ–ä¸ºï¼š$||\epsilon - \epsilon_\theta(x_t, t)||^2$ã€‚å³ï¼š**è®­ç»ƒç¥ç»ç½‘ç»œå»é¢„æµ‹ï¼ˆå¹¶å‡å»ï¼‰å›¾åƒä¸­çš„å™ªå£°**


**Pseudocode:**
![[Pasted image 20251109155826.png]]
Algorithm 1: Training (è®­ç»ƒç®—æ³•)

1. ä»æ•°æ®é›†ä¸­éšæœºé‡‡æ ·ä¸€å¼ å¹²å‡€çš„å›¾ç‰‡ $x_0$ã€‚
    
2. éšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ $t$ï¼ˆä» 1 åˆ° $T$ï¼‰ã€‚
    
3. éšæœºé‡‡æ ·ä¸€ä¸ªé«˜æ–¯å™ªå£° $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ã€‚
    
4. **æ ¸å¿ƒæ­¥éª¤**: è®¡ç®—æ¢¯åº¦ä¸‹é™ã€‚
    
    - æ¨¡å‹çš„è¾“å…¥æ˜¯ï¼šåŠ å™ªåçš„å›¾ç‰‡ $\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$ï¼ˆå³ $x_t$ï¼‰ å’Œæ—¶é—´æ­¥ $t$ã€‚
        
    - æ¨¡å‹çš„ç›®æ ‡æ˜¯ï¼šé¢„æµ‹å‡ºåˆšæ‰åŠ è¿›å»çš„é‚£ä¸ªå™ªå£° $\epsilon$ã€‚
        
    - æŸå¤±å‡½æ•°æ˜¯é¢„æµ‹å™ªå£°å’ŒçœŸå®å™ªå£°ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚
        

Algorithm 2: Sampling (é‡‡æ ·ç®—æ³•)

è¿™æ˜¯ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–°å›¾ç‰‡çš„è¿‡ç¨‹ï¼š

1. ä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒé‡‡æ ·ä¸€ä¸ªçº¯å™ªå£° $x_T \sim \mathcal{N}(0, \mathbf{I})$ã€‚
    
2. ä» $t = T$ åˆ° $1$ è¿›è¡Œå¾ªç¯ï¼ˆå€’åºå»å™ªï¼‰ï¼š
    
    - é‡‡æ ·ä¸€ä¸ªé¢å¤–çš„å™ªå£° $z$ï¼ˆå¦‚æœ $t > 1$ï¼‰ï¼Œç”¨äºå¢åŠ éšæœºæ€§ï¼ˆLangevin åŠ¨åŠ›å­¦ï¼‰ã€‚
        
    - **æ ¸å¿ƒæ­¥éª¤**: æ›´æ–° $x_{t-1}$ã€‚å…¬å¼é‡Œçš„çº¢è‰²æ¡†éƒ¨åˆ† $\frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t))$ å°±æ˜¯æ¨¡å‹é¢„æµ‹çš„å»å™ªåçš„å‡å€¼ $\mu_\theta(x_t, t)$ã€‚
        
    - ç®€å•ç†è§£å°±æ˜¯ï¼š**å½“å‰å›¾åƒå‡å»æ¨¡å‹é¢„æµ‹çš„å™ªå£°ï¼ˆæŒ‰ä¸€å®šæ¯”ä¾‹ï¼‰ï¼Œå†åŠ ä¸Šä¸€ç‚¹ç‚¹éšæœºæ‰°åŠ¨**ã€‚
        
3. å¾ªç¯ç»“æŸï¼Œè¿”å›æœ€ç»ˆç”Ÿæˆçš„å›¾ç‰‡ $x_0$ã€‚


---
# Noise Scheduler
## Linear schedule
$$\bar{\alpha}_t=\Pi^t_{s=0}(1-\beta_s)$$
- $\beta_s$ is constant
![[Pasted image 20251109155207.png]]

---
## Cosine schedule
$$\bar{\alpha}=\frac{f(t)}{f(0)}$$
- $f(t)=cos(\frac{\frac{t}{T}+S}{1+S}\times \frac{\pi}{2})^2$
![[Pasted image 20251109155216.png]]

---
# Latent Diffusion Model
- Perceptual image compression via Encoder $ğœ€$ and Decoder $ğ’Ÿ$
- Latent diffusion process with denoising U-Net $ğœ–_ğœƒ$
- Conditioning mechanism so that we can generate images by a prompt

![[Pasted image 20251109155616.png]]