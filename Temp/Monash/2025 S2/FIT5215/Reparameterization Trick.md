---
date: 2025-11-09
author:
  - Siyuan Liu
tags:
  - FIT5215
---
**重参数化技巧 (Reparameterization Trick)** 是一种极其巧妙的数学手段，它解决了在神经网络中对**随机采样过程**进行求导（反向传播）的难题。它最著名的应用是在变分自编码器 (VAE) 中

### 1. 核心问题：随机性阻断了梯度

假设神经网络中间有一步需要从某个分布中随机采样一个值 $z$。

比如在 VAE 中，编码器输出了均值 $\mu$ 和方差 $\sigma^2$，我们需要从高斯分布 $N(\mu, \sigma^2)$ 中采样得到潜变量 $z$，然后再传给解码器。

- **正向传播 (Forward)**：很简单，直接算 $\mu, \sigma$，然后用随机数生成器得到 $z$。
    
- **反向传播 (Backward)**：问题来了！**“采样”这个操作是不可导的**。
    
    - 梯度想要从解码器传回编码器，必须经过 $z$。
        
    - 但是 $z$ 是随机“蹦”出来的，它和 $\mu, \sigma$ 之间没有明确的确定性函数关系能让我们求导（$\frac{\partial z}{\partial \mu}$ 和 $\frac{\partial z}{\partial \sigma}$ 无法直接计算，因为 $z$ 的值里包含了无法追踪的随机噪声）。
        
    - 这就像电路中间断开了一样，梯度传不过去。
        

### 2. 解决方案：重参数化技巧

这个技巧的核心思想是：**把随机性从网络的主干道上“剥离”出去，放到一个旁路分支上。**

它将随机变量 $z \sim N(\mu, \sigma^2)$ 改写成一个确定性的函数：

$$z = \mu + \sigma \cdot \epsilon$$

其中：

- $\mu$ 和 $\sigma$ 是网络计算出来的确定性变量（主干道）。
    
- **$\epsilon \sim N(0, 1)$** 是从标准正态分布中采样得到的**纯噪声**（旁路）。
    

### 3. 为什么这样就解决了问题？

让我们再看看现在的计算图：

1. 我们先从外部（旁路）采样一个与网络参数无关的噪声 $\epsilon$。
    
2. 然后执行确定性的乘法和加法：$z = \mu + \sigma \times \epsilon$。
    

现在进行反向传播时：

- 我们想求 $z$ 对 $\mu$ 的导数：$\frac{\partial z}{\partial \mu} = 1$ （完全可导！）
    
- 我们想求 $z$ 对 $\sigma$ 的导数：$\frac{\partial z}{\partial \sigma} = \epsilon$ （完全可导！）
    

**梯度现在可以顺畅地通过 $z$ 流回 $\mu$ 和 $\sigma$，进而更新编码器的参数了。** 那个不可导的随机采样过程被我们成功地“甩锅”给了常数输入 $\epsilon$，而 $\epsilon$ 不需要更新梯度，所以它不可导也没关系。