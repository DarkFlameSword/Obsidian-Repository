---
date: 2025-08-04
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - base
---
# Saturated Activate Function 饱和激活函数
![[Pasted image 20250804160215.png]]
## `Sigmoid`
![[Pasted image 20250804154105.png]]
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
==优:==
- Sigmoid 函数的输出范围是 0 到 1。非常适合作为模型的输出函数用于输出一个0~1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。
- 梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度
==缺:==
- 容易造成梯度消失。我们从导函数图像中了解到sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。这样几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新，这就叫梯度消失。除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意。如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习
- 函数输出不是以 0 为中心的，梯度可能就会向特定方向移动，从而降低权重更新的效率
- Sigmoid 函数执行指数运算，计算机运行得较慢，比较消耗计算资源
## `Tanh`
![[Pasted image 20250804154147.png]]
$$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$
==优:==
- tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；
- 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。
==缺:==
- 仍然存在梯度饱和的问题
- 依然进行的是指数运算
# Unsaturated Activate Function
## `ReLU` 
![[Pasted image 20250804154218.png]]
$$ \mathrm{ReLU}(x) = \max(0, x) $$
==优:==
 - ReLU解决了梯度消失的问题，当输入值为正时，神经元不会饱和
- 由于ReLU线性、非饱和的性质，在SGD中能够快速收敛
- 计算复杂度低，不需要进行指数运算
==缺:==
- 与Sigmoid一样，其输出不是以0为中心的
- Dead ReLU 问题。当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新

## `Leaky ReLU`
![[Pasted image 20250804154446.png]]
$$ \mathrm{Leaky ReLU}(x) = \max(\alpha x, x) $$==优:==
- 解决了ReLU输入值为负时神经元出现的死亡的问题
- Leaky ReLU线性、非饱和的性质，在SGD中能够快速收敛
- 计算复杂度低，不需要进行指数运算
==缺:==
- 函数中的α，需要通过先验知识人工赋值（一般设为0.01）
- 有些近似线性，导致在复杂分类中效果不好。
==Attention:==
1. `ɑ`一般取1/5.5

## `Parametric ReLU/PReLU`
![[Pasted image 20250804155221.png]]
$f(x) = \begin{cases} x, & x \geq 0 \\ a x, & x < 0 \end{cases}$
==优:==
- 解决ReLU带来的神经元坏死的问题, 与Leaky ReLU激活函数不同的是，PRelu激活函数负半轴的斜率参数**α** 是通过学习得到的，而不是手动设置的恒定值
==缺:==

## `ELU`
![[Pasted image 20250804155451.png]]
$$ \mathrm{ELU}(x) = \begin{cases} x, & x > 0 \\ \alpha(e^x - 1), & x \leq 0 \end{cases} $$
==优:==
- ELU试图将激活函数的输出均值接近于零，使正常梯度更接近于单位自然梯度，从而加快学习速度
- ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息
==缺:==
- 计算的时需要计算指数，计算效率低
## `SeLU`
![[Pasted image 20250804155914.png]]
$$
\mathrm{SeLU}(x) = 
\lambda
\begin{cases}
x & \text{if } x > 0 \\
\alpha (e^x - 1) & \text{if } x \leq 0
\end{cases}
$$
==优:==
- 它的值有正有负：在整个ReLU的family里里面，除了一开始最原始的ReLU以外都有负值，该函数也贯彻了这个特性
- SELU激活函数是在自归一化网络中定义的，通过调整均值和方差来实现内部的归一化，这种内部归一化比外部归一化更快，这使得网络能够更快得收敛
- SELU 允许构建一个映射 g，其性质能够实现 SNN（自归一化神经网络）
==缺:==

## `Swish`
![[Pasted image 20250804160431.png]]
$$ \mathrm{Swish}(x) = x \cdot \sigma(x) $$
==优:==
- Swish激活函数**无界性**有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和
- **有界性**也是有优势的，因为有界激活函数可以具有很强的正则化(防止过拟合， 进而增强泛化能力)，并且较大的负输入问题也能解决
- Swish激活函数在`x=0`附近更为平滑，而非单调的特性增强了输入数据和要学习的权重的表达能力
==缺:==
## `Softmax`
![[Pasted image 20250804160702.png]]
$$ \mathrm{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} $$
==优:==
Softmax函数常在神经网络输出层充当激活函数，将输出层的值通过激活函数映射到0-1区间，将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大
==缺:==
