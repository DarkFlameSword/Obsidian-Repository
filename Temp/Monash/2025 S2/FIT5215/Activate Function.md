---
date: 2025-08-04
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - base
---
# Saturated Activate Function 饱和激活函数
![[Pasted image 20250804160215.png]]
## `Sigmoid`
![[Pasted image 20250804154105.png]]
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
==优:==
- Sigmoid 函数的输出范围是 0 到 1。非常适合作为模型的输出函数用于输出一个0~1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。
- 梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度
==缺:==
- 容易造成梯度消失。我们从导函数图像中了解到sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。这样几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新，这就叫梯度消失。除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意。如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习
- 函数输出不是以 0 为中心的，梯度可能就会向特定方向移动，从而降低权重更新的效率
- Sigmoid 函数执行指数运算，计算机运行得较慢，比较消耗计算资源
## `Tanh`
![[Pasted image 20250804154147.png]]
$$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$
==优:==
- tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；
- 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。
==缺:==
- 仍然存在梯度饱和的问题
- 依然进行的是指数运算
# Unsaturated Activate Function
## `ReLU` 
![[Pasted image 20250804154218.png]]
$$ \mathrm{ReLU}(x) = \max(0, x) $$
==优:==
 - ReLU解决了梯度消失的问题，当输入值为正时，神经元不会饱和
- 由于ReLU线性、非饱和的性质，在SGD中能够快速收敛
- 计算复杂度低，不需要进行指数运算
==缺:==
- 与Sigmoid一样，其输出不是以0为中心的
- Dead ReLU 问题。当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新

## `Leaky ReLU`
![[Pasted image 20250804154446.png]]
$$ \mathrm{Leaky ReLU}(x) = \max(\alpha x, x) $$==优:==
- 解决了ReLU输入值为负时神经元出现的死亡的问题
- Leaky ReLU线性、非饱和的性质，在SGD中能够快速收敛
- 计算复杂度低，不需要进行指数运算
==缺:==
- 函数中的α，需要通过先验知识人工赋值（一般设为0.01）
- 有些近似线性，导致在复杂分类中效果不好。
==Attention:==
1. `ɑ`一般取1/5.5

## `Parametric ReLU/PReLU`
![[Pasted image 20250804155221.png]]
$f(x) = \begin{cases} x, & x \geq 0 \\ a x, & x < 0 \end{cases}$
==优:==
- 解决ReLU带来的神经元坏死的问题, 与Leaky ReLU激活函数不同的是，PRelu激活函数负半轴的斜率参数**α** 是通过学习得到的，而不是手动设置的恒定值
==缺:==

## `ELU`
![[Pasted image 20250804155451.png]]
$$ \mathrm{ELU}(x) = \begin{cases} x, & x > 0 \\ \alpha(e^x - 1), & x \leq 0 \end{cases} $$
==优:==
- ELU试图将激活函数的输出均值接近于零，使正常梯度更接近于单位自然梯度，从而加快学习速度
- ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息
==缺:==
- 计算的时需要计算指数，计算效率低
## `SeLU`
![[Pasted image 20250804155914.png]]
$$
\mathrm{SeLU}(x) = 
\lambda
\begin{cases}
x & \text{if } x > 0 \\
\alpha (e^x - 1) & \text{if } x \leq 0
\end{cases}
$$
==优:==
- 它的值有正有负：在整个ReLU的family里里面，除了一开始最原始的ReLU以外都有负值，该函数也贯彻了这个特性
- SELU激活函数是在自归一化网络中定义的，通过调整均值和方差来实现内部的归一化，这种内部归一化比外部归一化更快，这使得网络能够更快得收敛
- SELU 允许构建一个映射 g，其性质能够实现 SNN（自归一化神经网络）
==缺:==

## `Swish`
![[Pasted image 20250804160431.png]]
$$ \mathrm{Swish}(x) = x \cdot \sigma(x) $$
==优:==
- Swish激活函数**无界性**有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和
- **有界性**也是有优势的，因为有界激活函数可以具有很强的正则化(防止过拟合， 进而增强泛化能力)，并且较大的负输入问题也能解决
- Swish激活函数在`x=0`附近更为平滑，而非单调的特性增强了输入数据和要学习的权重的表达能力
==缺:==
## `Softmax`
![[Pasted image 20250804160702.png]]
$$ \mathrm{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} $$
==优:==
Softmax函数常在神经网络输出层充当激活函数，将输出层的值通过激活函数映射到0-1区间，将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大
==缺:==

**作用**: `Softmax`的作用就像一个“转换器”，它能把这些原始分数(**Logits** / **Scores**)，转换成每个选项最终的“机率”(1. 每个选项的机率都在 0% 到 100% 之间 2.所有选项的机率加起来正好是 100%)

**举例:**
评委打出的原始分数如下：
- 选手A (猫): **3.2** 分
- 选手B (狗): **1.3** 分
- 选手C (鸟): **0.9** 分

**第1步：取指数 (Exponentiation)**

为了让所有分数都变成正数，并且拉开差距（让高分的优势更明显），我们对每个分数都取自然常数 `e` 的指数。

- **猫**：`e^3.2 ≈ 24.53`
- **狗**：`e^1.3 ≈ 3.67`
- **鸟**：`e^0.9 ≈ 2.46`

现在，分数都变成了正数，而且猫的优势被放大了。

**第2步：归一化 (Normalization)**

把上面得到的所有新分数加起来，得到一个总和。然后，用每个选手的新分数除以这个总和，就得到了每个选手的“得票率”。

- **总和** = `24.53 + 3.67 + 2.46 = 30.66`

现在计算每个类别的最终概率：

- **P(猫)** = `24.53 / 30.66 ≈ 0.80` (80% 的概率是猫)
- **P(狗)** = `3.67 / 30.66 ≈ 0.12` (12% 的概率是狗)
- **P(鸟)** = `2.46 / 30.66 ≈ 0.08` (8% 的概率是鸟)

**计算完成！**

我们现在得到了一组清晰的概率分布：`[0.80, 0.12, 0.08]`。

- 它们都在 0 和 1 之间。
- 它们的总和是 `0.80 + 0.12 + 0.08 = 1.0`。

现在我们可以非常自信地说：**模型认为这张图片有 80% 的可能性是猫。**