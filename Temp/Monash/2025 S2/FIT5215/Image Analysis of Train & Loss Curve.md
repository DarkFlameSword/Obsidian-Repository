---
date: 2025-08-07
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - summary
---
# Gradient vanishing
![[Pasted image 20250807174530.png]]
**定义:**
Gradients get smaller and smaller as the algorithm progresses down to the lower layers

**表现:**
- **Loss 曲线:**
    - 训练集 Loss 下降缓慢
    - 训练集 Loss 在后期几乎不再下降，趋于平缓
    - 验证集 Loss 也呈现相似的趋势
- **Accuracy 曲线:**
    - 训练集 Accuracy 提升缓慢
    - 训练集 Accuracy 在后期几乎不再提升，趋于平缓
    - 验证集 Accuracy 也呈现相似的趋势

**多发场景:**
- activate function 使用的是`Sigmoid`,`tanh`

**解决办法:**
- 避免使用`Sigmoid`,`tanh`
- 选择合适的W,b初始化
    - `Xavier`/`Glorot` 初始化（适合 `tanh`/线性模型）
    - He 初始化（适合`ReLU`)
- 通过保持中间特征的分布稳定，避免梯度消失/爆炸
    - Batch Normalization (BN) —— 训练加速、梯度稳定
    - Layer Normalization (LN) —— 特别适合 RNN/Transformer
    - Group Normalization (GN)、Instance Norm 等
- 残差连接（Residual Connections）
    - `ResNet` 中的 shortcut/skip connection 可以让梯度绕过深层传播，极大缓解梯度消失/爆炸
---
# Gradient exploding
![[Pasted image 20250807173729.png]]
**定义:**
The gradients can grow bigger and bigger, so many layers get insanely large weight updates, and the training diverges

**表现:**
- **Loss 曲线:**
    - 训练集 Loss 在初期迅速增加
    - 训练集 Loss 出现明显的震荡，可能包含 NaN 值
    - 验证集 Loss 也可能受到影响，出现震荡
- **Accuracy 曲线:**
    - 训练集 Accuracy 在初期迅速下降
    - 训练集 Accuracy 出现明显的震荡
    - 验证集 Accuracy 也可能受到影响，出现下降

**多发场景:**
- 深层神经网络
- RNN/LSTM/Bidirectional RNN

**解决办法:**
- 梯度裁剪 (Gradient Clipping)
    - 在反向传播得到梯度后，若梯度范数超过设定阈值，就按比例缩放到合理范围
    - 常用于RNNs,NLP, 但是在CNNs中不常用
- 合理的学习率
    - 可以使用 **学习率调度器 (scheduler)** 或 **自适应优化器** (如 Adam, RMSprop)
- 通过保持中间特征的分布稳定，避免梯度消失/爆炸
    - Batch Normalization (BN) —— 训练加速、梯度稳定
    - Layer Normalization (LN) —— 特别适合 RNN/Transformer
    - Group Normalization (GN)、Instance Norm 等
- 残差连接（Residual Connections）
    - `ResNet` 中的 shortcut/skip connection 可以让梯度绕过深层传播，极大缓解梯度消失/爆炸
---
# Overfitting
![[Pasted image 20250824192640.png]]
![[Pasted image 20250824192802.png]]
![[Pasted image 20250824204807.png]]
**表现:**
At a certain point, the train loss still decreases, while the valid loss starts increasing

**解决方法:**
1. **增加训练数据**
    - 数据增强 (Data Augmentation): 通过对现有训练数据进行各种变换（例如旋转、翻转、缩放、裁剪、平移、添加噪声等）来生成新的训练样本，从而扩大数据集的规模
    - 收集更多真实数据: 如果条件允许，尽量收集更多真实的数据，这是解决过拟合最有效的方法之一

2. **简化模型**
    - 减少模型层数: 减少神经网络的层数，降低模型的深度
    - 减少神经元数量: 减少每层神经网络的神经元数量，降低模型的宽度
    - 参数共享: 在某些网络结构中，可以采用参数共享的方式来减少模型的参数量
    - 选择更简单的模型结构: 例如，可以考虑使用线性模型、决策树等更简单的模型，而不是复杂的神经网络

3. **正则化**
    - L1 正则化 (Lasso): 在损失函数中添加 L1 正则项，鼓励模型学习稀疏的权重，即让更多的权重变为 0，从而减少模型的复杂度
    - L2 正则化 (Ridge): 在损失函数中添加 L2 正则项，限制权重的平方和，使得权重更加平滑，从而提高模型的泛化能力
    - Dropout: 在训练过程中，随机地丢弃一部分神经元，强制模型学习更加鲁棒的特征，避免模型过度依赖某些特定的神经元

4. **早停策略 (Early Stopping)**
    - 监控验证集性能: 在训练过程中，定期评估模型在验证集上的性能
    - 提前停止训练: 当验证集上的性能开始下降时，停止训练，避免模型继续学习训练数据中的噪声

5. **数据清洗**
    - 去除噪声数据: 检查训练数据，去除错误标签、异常值或不一致的数据
    - 数据平滑: 对数据进行平滑处理，减少噪声的影响

6. **集成学习**
    - Bagging: 通过对训练数据进行多次采样，训练多个模型，然后对它们的预测结果进行平均或投票，从而提高模型的泛化能力
    - Boosting: 通过迭代地训练多个模型，每个模型都关注之前模型预测错误的样本，从而提高模型的准确率和泛化能力
---
# Underfitting
![[E751AE193BB8D450C419CACD708E93C2.png]]
**表现:**
- **Loss 曲线:**
    - 训练集 Loss 较高，且下降缓慢
    - 验证集 Loss 较高，且下降缓慢，与训练集 Loss 曲线接近
    - 两条曲线在整个训练过程中都保持较高的 Loss 值
- **Accuracy 曲线:**
    - 训练集 Accuracy 较低，且提升缓慢
    - 验证集 Accuracy 较低，且提升缓慢，与训练集 Accuracy 曲线接近
    - 两条曲线在整个训练过程中都保持较低的 Accuracy 值


**解决方法:**
1. **增加模型复杂度**
- 增加网络层数 (Deeper Network): 增加神经网络的层数，使模型能够学习更复杂的特征表示
- 增加每层神经元数量 (Wider Network): 增加每层网络的神经元数量，提高模型的容量
- 使用更复杂的模型结构: 例如，将线性模型替换为非线性模型，或者使用更高级的神经网络结构，如残差网络 (ResNet)、Transformer 等

2. **延长训练时间**
- 增加训练轮数 (Epochs): 增加训练轮数，使模型能够充分学习到数据中的模式
- 监控训练过程: 监控训练集和验证集的损失函数值，确保模型在训练过程中持续学习

3. **减少正则化**
- 减小 L1/L2 正则化系数: 减小 L1 或 L2 正则化系数，降低模型受到的约束，使其能够更好地拟合训练数据
- 减少 Dropout 比例: 减少 Dropout 比例，使更多的神经元参与训练，提高模型的容量

4. **调整学习率**
- 使用更小的学习率: 如果学习率过高导致模型无法收敛，可以尝试使用更小的学习率
- 使用学习率衰减策略: 在训练过程中逐渐降低学习率，例如使用 step decay、exponential decay 或 cosine annealing 等策略
- 自适应学习率算法: 使用自适应学习率算法，例如 Adam、RMSprop 或 Adagrad 等，这些算法可以自动调整学习率

5. **改进数据预处理**
- 数据归一化/标准化: 对数据进行归一化或标准化，使数据的分布更加均匀，提高模型的训练效率
- 处理数据偏差: 如果数据存在偏差，可以尝试使用重采样或重加权等方法来平衡数据

6. **优化算法选择**
- 尝试不同的优化算法: 选择适合当前模型和数据的优化算法，例如 Adam、SGD 或 RMSprop 等

7. **使用集成学习**
- Bagging: 使用 Bagging 方法训练多个模型，然后对它们的预测结果进行平均或投票，从而提高模型的泛化能力
- Boosting: 使用 Boosting 方法训练多个模型，每个模型都关注之前模型预测错误的样本，从而提高模型的准确率和泛化能力


# 损失函数震荡
![[Pasted image 20250807175412.png]]
**原因:**
1. **学习率过大：**
    - **跳跃性更新：** 学习率决定了每次迭代中参数更新的幅度。 如果学习率过大，模型可能会在损失函数的表面上跳跃，难以收敛到最优解，导致损失函数值震荡。
2. **Batch Size 过小：**
    - **梯度估计不稳定：** Batch Size 决定了每次迭代中使用的样本数量。 如果 Batch Size 过小，每次迭代计算出的梯度可能无法准确地代表整个数据集的梯度，导致梯度估计不稳定，从而导致损失函数值震荡。
3. **数据集存在噪声：**
    - **异常样本干扰：** 如果数据集中存在噪声（例如异常值、错误标签），模型在训练过程中可能会受到这些噪声的影响，导致损失函数值震荡。
4. **模型复杂度过高：**
    - **过度拟合：** 如果模型过于复杂，可能会过度拟合训练数据，导致模型在训练集上表现很好，但在验证集上表现很差。 在这种情况下，损失函数值可能会在训练过程中震荡。
5. **优化算法选择不当：**
    - **不合适的算法：** 某些优化算法（例如传统的梯度下降算法）可能难以处理损失函数表面上的局部最小值或鞍点，导致损失函数值震荡。
6. **梯度爆炸：**
    - **数值不稳定：** 梯度爆炸会导致梯度值变得非常大，从而导致损失函数值震荡。
7. **激活函数选择不当：**
    - **梯度消失/爆炸：** 某些激活函数（例如 Sigmoid 和 Tanh）在输入值很大或很小时，梯度会变得非常小或非常大，从而导致损失函数值震荡。
8. **数据预处理不当：**
    - **未归一化/标准化：** 如果输入数据没有进行归一化或标准化，可能会导致某些层的输入值很大，从而导致损失函数值震荡。
9. **训练数据和验证数据分布不一致：**
    - **泛化能力差：** 如果训练数据和验证数据的分布不一致，模型在训练集上学习到的模式可能无法很好地泛化到验证集上，从而导致损失函数值震荡。

**解决方法:**
1. **调整学习率：**
    - **减小学习率：** 这是最常见的解决方法。 尝试使用更小的学习率，例如将学习率降低 10 倍。
    - **学习率衰减：** 使用学习率衰减策略，例如 step decay、exponential decay 或 cosine annealing 等，在训练过程中逐渐降低学习率。 这可以使得模型在训练初期快速收敛，在训练后期更加稳定。
    - **自适应学习率算法：** 使用自适应学习率算法，例如 Adam、RMSprop 或 Adagrad 等，这些算法可以自动调整学习率，从而避免损失函数值震荡。
2. **调整 Batch Size：**
    - **增大 Batch Size：** 增大 Batch Size 可以使得每次迭代计算出的梯度更加准确地代表整个数据集的梯度，从而减少梯度估计的方差，降低损失函数值的震荡。 但过大的 Batch Size 可能会导致内存不足，需要根据实际情况进行调整。
3. **选择合适的优化算法：**
    - **尝试不同的优化算法：** 选择适合当前模型和数据的优化算法，例如 Adam、RMSprop 或 SGD 等。 不同的优化算法具有不同的特性，可以根据实际情况进行选择。
4. **梯度裁剪 (Gradient Clipping)：**
    - **限制梯度大小：** 如果损失函数震荡是由于梯度爆炸导致的，可以使用梯度裁剪方法限制梯度的大小，从而避免梯度爆炸。
5. **使用合适的激活函数：**
    - **避免梯度消失/爆炸：** 选择合适的激活函数，例如 ReLU 及其变体，可以避免梯度消失和梯度爆炸问题，从而提高模型的训练稳定性。
6. **检查损失函数定义**
