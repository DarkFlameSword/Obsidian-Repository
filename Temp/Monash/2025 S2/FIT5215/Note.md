---
date: 2025-07-27
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - note
---
# Vector

![[Pasted image 20250727164025.png]]
==Attention:==
1. 一般用变量表示的向量默认是列向量, 横向量需要使用转置符号`T`标明
## Multiplication

![[Pasted image 20250727164902.png]]
## Transpose
![[Pasted image 20250727165020.png]]
## p-norm/范数
![[Pasted image 20250727165647.png]]
### The Length of Vector
当p=2的时候也叫Frobenius范数, 一般我们求矩阵长度使用的就是该范数

### Distance between Two Vectors
![[Pasted image 20250727170036.png]]
### The Angel between Two Vectors
![[Pasted image 20250727170151.png]]
# Matrix 2D
![[Pasted image 20250727171416.png]]
==Attention==
1. AB矩阵相乘, 最后的结果矩阵的shape会取A的行数B的列数
2. 第一个矩阵 (A) 的列数必须等于第二个矩阵 (B) 的行数, 否则不能相乘
# Kullback-Leibler (KL) divergence
个人理解: KL散度用来计算, 当前模型距离真实案例的偏差值

**D_KL(P || Q) = Σ P(x) * log₂( P(x) / Q(x) )**
==![[Pasted image 20250728164017.png]]==

![[Pasted image 20250727172836.png]]
# Entropy of the distribution
个人理解: 衡量的是一个随机事件或一个概率分布的**不确定性**或**混乱程度**, 也就是要搞清楚一个随机事件的最终结果，平均需要多少信息量（通常用比特`bit`来衡量）
- **熵越高**，代表系统越混乱，结果越不可预测
- **熵越低**，代表系统越有序，结果越容易预测

**H(X) = - Σ p(x) * log₂(p(x))**
==![[Pasted image 20250728164017.png]]

![[Pasted image 20250727173520.png]]
# Cross-entropy (CE)
个人理解: 计算预测结果与真实结果之间的差距, CE越大预测结果越离谱

**H(p, q) = - Σ p(x) * log₂(q(x))**
==![[Pasted image 20250728164017.png]]

![[Pasted image 20250727174454.png]]
==Attention:==
1. CrossEntropy(p, q) = Entropy(p) + KL_Divergence(p || q) **在机器学习中, 真实数据分布 `p` 是固定的, 所以 `Entropy(p)` 是一个常数, 所以最小化交叉熵就等价于最小化KL散度**

# `Softmax`

**作用**: `Softmax`的作用就像一个“转换器”，它能把这些原始分数(**Logits** / **Scores**)，转换成每个选项最终的“机率”(1. 每个选项的机率都在 0% 到 100% 之间 2.所有选项的机率加起来正好是 100%)

## 计算
评委打出的原始分数如下：
- 选手A (猫): **3.2** 分
- 选手B (狗): **1.3** 分
- 选手C (鸟): **0.9** 分

**第1步：取指数 (Exponentiation)**

为了让所有分数都变成正数，并且拉开差距（让高分的优势更明显），我们对每个分数都取自然常数 `e` 的指数。

- **猫**：`e^3.2 ≈ 24.53`
- **狗**：`e^1.3 ≈ 3.67`
- **鸟**：`e^0.9 ≈ 2.46`

现在，分数都变成了正数，而且猫的优势被放大了。

**第2步：归一化 (Normalization)**

把上面得到的所有新分数加起来，得到一个总和。然后，用每个选手的新分数除以这个总和，就得到了每个选手的“得票率”。

- **总和** = `24.53 + 3.67 + 2.46 = 30.66`

现在计算每个类别的最终概率：

- **P(猫)** = `24.53 / 30.66 ≈ 0.80` (80% 的概率是猫)
- **P(狗)** = `3.67 / 30.66 ≈ 0.12` (12% 的概率是狗)
- **P(鸟)** = `2.46 / 30.66 ≈ 0.08` (8% 的概率是鸟)

**计算完成！**

我们现在得到了一组清晰的概率分布：`[0.80, 0.12, 0.08]`。

- 它们都在 0 和 1 之间。
- 它们的总和是 `0.80 + 0.12 + 0.08 = 1.0`。

现在我们可以非常自信地说：**模型认为这张图片有 80% 的可能性是猫。**
# AI Model Types

## Supervised Learning
**核心思想：** 通过**有答案的“练习题”**来学习。

把它想象成一个学生（机器学习模型）在学习。我们为他提供大量的练习题（输入数据 `X`），并且**每一道题都附带了标准答案**（输出标签 `y`）。学生的目标就是学习从题目到答案之间的规律和映射关系。

**关键特征：**

- **使用“已标记”的数据 (Labeled Data)**：数据集中的每个样本都包含两部分：**特征 (Features)** 和与之对应的 **标签 (Label)** 或 **目标 (Target)**。
- **目标**：学习一个函数 `f`，使得对于新的、从未见过的数据 `X_new`，模型能够预测出正确的标签 `y_new`，即 `y_new ≈ f(X_new)`。
### Classification
**核心思想：** 预测一个**离散的类别标签**。简单说，就是做“选择题”。

模型的任务是将输入数据分到一个预先定义好的类别中。输出的结果是有限的、不连续的类别。

**目标：** 输出是一个**类别 (Class)**。
### Regression
**核心思想：** 预测一个**连续的数值**。简单说，就是做“填空题”，填一个具体的数字。

模型的任务是基于输入数据，预测一个精确的、连续的输出值。

**目标：** 输出是一个**数值 (Value)**。
## Reinforce Learning

# Performance Metrics

### Accuracy
### Recall
### Precision
### F-score
