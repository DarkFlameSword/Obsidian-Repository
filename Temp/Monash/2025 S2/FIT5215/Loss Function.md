---
date: 2025-08-07
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - base
---
# Multi-class Classification
## `Kullback-Leibler (KL) divergence`
个人理解: KL散度用来计算, 当前模型距离真实案例的偏差值
$$
D_{KL}(P || Q) = \sum_{x} P(x) \cdot \log \left( \frac{P(x)}{Q(x)} \right)
$$
```
log: 对数函数。对数的底 (base) 决定了熵的单位:
    - 如果底为 2 (log₂), 熵的单位是 **比特** `bit`这是信息论和计算机科       学中最常用的
    - 如果底为 e (自然对数 ln), 熵的单位是 **奈特** `nat`
    - 如果底为 10 (log₁₀), 熵的单位是 **哈特** `hartley`
```
- P(x) 是事件x在真实分布中的概率
- Q(x) 是事件x在模型分布中的概率
- log₂ 表示我们用“比特”来衡量信息量

**公式解读:**
- P(x) / Q(x) : 这个比率衡量了模型Q在事件x上的预测有多“离谱”。
    - 如果 Q(x) 低估了 P(x) (即真实发生概率高，但模型预测概率低)，这个比值就很大，表示“意外”很大。
    - 如果 Q(x) 高估了 P(x), 比值就小于1。
    - 如果预测完美 P(x) = Q(x), 比值为1, log(1)=0, 没有意外, 不产生散度。
- P(x) 作为权重: 我们更关心那些真实世界中经常发生的事件 (P(x)高) 的预测准确性。如果模型在一个很罕见的事件上预测错了，影响不大；但在一个常见事件上预测错了，代价就很高。
## `Cross-entropy (CE)`
个人理解: 计算预测结果与真实结果之间的差距, CE越大预测结果越离谱; CE越接近0, 则说明该事件越接近真实概率

### **Classification Cross-Entropy Loss**
$$
\text{BCE} = -\frac{1}{n} \sum_{i=1}^n \Big[ p_i \log(q_i) + (1 - y_i)\log(1 - \hat{y}_i) \Big]

$$
- $p_i$: 真实标签，只能是 **0 或 1**。
- $q_i$: 模型预测的概率，通常是经过 `Sigmoid` 函数输出的，值在 (0, 1) 之间，表示样本为类别 1 的概率。

**真实标签 (The Truth: p)**
对于一张确认认为猫的图片，它的“真实”概率分布 p 是非常确定的。我们通常用 one-hot 编码 来表示:
```
p = [是猫, 不是猫] = [1, 0]
```
这代表：这张图片100%是猫，0%是狗。 这个 p 是我们的基准和事实。

**模型的预测 (The Prediction: q)**
模型在看到这张图片后，会输出一个它自己预测的概率分布 q。我们来看三种情况：
- 情况A (预测得很好): 模型非常有信心地认为这是猫。
    - q1 = [0.9, 0.1] (90%可能是猫, 10%可能是狗)
- 情况B (预测得很差): 模型非常有信心地认为这是狗。
    - q2 = [0.1, 0.9] (10%可能是猫, 90%可能是狗)
- 情况C (不确定): 模型完全不确定，觉得两种可能性差不多。
    - q3 = [0.5, 0.5] (50%可能是猫, 50%可能是狗)

**特别注意:** 因为真实标签 p 是one-hot编码, p(x) 的值要么是1，要么是0。这会极大地简化计算! 只有真实标签为1的那一项会保留下来。
```
H(p, q) = - [ p(猫)*log(q(猫)) + p(狗)*log(q(狗)) ] 
H(p, q) = - [ 1 * log(q(猫)) + 0 * log(q(狗)) ] 
H(p, q) = -log(q(猫))
```

所以，对于这张猫的图片，交叉熵损失就简化成了 模型预测它是猫的概率的负对数

==Attention:==
- 如果真实标签 `y=1`: 公式简化为 `-log(ŷ)`。为了让 Loss 变小，`log(ŷ)` 就要变大，这意味着 `ŷ` 必须**趋近于 1
- 如果真实标签 `y=0`: 公式简化为 `-log(1 - ŷ)`。为了让 Loss 变小，`log(1 - ŷ)` 就要变大，这意味着 `(1 - ŷ)` 必须趋近于 1，也就是 `ŷ` 必须**趋近于 0**
- `CrossEntropy(p, q) = Entropy(p) + KL_Divergence(p || q)` 在机器学习中, 真实数据分布 `p` 是固定的, 所以 `Entropy(p)` 是一个常数, 所以最小化交叉熵就等价于最小化KL散度

### **Categorical Cross-Entropy**
$$
\text{CCE} = - \frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})

$$
- `yᵢ,c`: 是一个 **One-Hot 编码**的向量。如果第 `i` 个样本的真实类别是 `c`，则 `yᵢ,c=1`，否则为 0。
- `ŷᵢ,c`: 模型预测的概率分布，通常是经过 `Softmax` 函数输出的，表示第 `i` 个样本属于类别 `c` 的概率。

==Attention:==
- 由于 One-Hot 编码的存在，对于每个样本 `i`，只有一个 `yᵢ,c` 是 1，其他都是 0。所以内层求和 `Σ` 会被简化





### `Entropy of the Distribution`
个人理解: 衡量的是一个随机事件或一个概率分布的**不确定性**或**混乱程度**, 也就是要搞清楚一个随机事件的最终结果，平均需要多少信息量（通常用比特`bit`来衡量）
- **熵越高**，代表系统越混乱，结果越不可预测
- **熵越低**，代表系统越有序，结果越容易预测

$$
H(p, q) = - \sum_{x} p(x) \cdot \log(p(x))
$$
==距离:==
在这场模拟比赛中，四匹马的实力完全相当，每匹马获胜的概率都是一样的。
- **概率分布 P:**
    - P(A 获胜) = 0.25
    - P(B 获胜) = 0.25
    - P(C 获胜) = 0.25
    - P(D 获胜) = 0.25
在比赛开始前，你对结果一无所知，充满了不确定性。这就是一个高熵系统

**信息熵的计算公式:**

$$
H(p, q) = - \sum_{x} p(x) \cdot \log_2(p(x))
$$

- p(x) 是事件 x 发生的概率。
- log₂ 意味着我们用“比特”来度量信息量。
- log₂(p(x)) 可以理解为当事件 x 发生时，你所获得的“信息量”或“惊喜程度”。概率越小的事情发生了，惊喜程度越大。
- - 号用来保证最终结果是正数 (因为概率小于1，其对数为负)。

**计算过程:** 
```
H(P) 
= - [ P(A)log₂(P(A)) + P(B)log₂(P(B)) + P(C)log₂(P(C)) + P(D)log₂(P(D)) ] 
= - [ 4 * (0.25 * log₂(0.25)) ] 
= - [ 4 * (0.25 * -2) ] (因为 log₂(0.25) = log₂(1/4) = -2 ) 
= - [ 4 * -0.5 ] 
= 2 比特
```


**结果解读:** 这个比赛的熵是 **2 比特**。这意味着，为了完全消除不确定性 (即准确知道哪匹马赢了)，你平均需要 **2 比特** 的信息。这很好理解，因为要从4个选项中确定一个，我们正好需要两位二进制数来编码 (例如: 00代表A, 01代表B, 10代表C, 11代表D)。
# Regression

## `L1 Norm Loss / Mean Absolute Error Loss (MAE)`
![[Pasted image 20250807170502.png]]
$$ L = \frac{1}{n} \sum_{i=1}^{n} |y_i - p_i|$$
`y_i` 是真实值，`p_i` 是模型预测值，`n` 是样本数量
## `L2 Norm Loss / Mean Squared Error Loss(MSE)`
![[Pasted image 20250807170451.png]]
$$ L = \frac{1}{n} \sum_{i=1}^{n} |y_i - p_i|^2$$
`y_i` 是真实值，`p_i` 是模型预测值，`n` 是样本数量
## ε−insensitive Loss



