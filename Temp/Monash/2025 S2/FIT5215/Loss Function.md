---
date: 2025-08-07
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - base
---
# Classification
## `Kullback-Leibler (KL) divergence`
个人理解: KL散度用来计算, 当前模型距离真实案例的偏差值
$$
D_{KL}(P || Q) = \sum_{x} P(x) \cdot \log_2 \left( \frac{P(x)}{Q(x)} \right)
$$
```
log: 对数函数。对数的底 (base) 决定了熵的单位:
    - 如果底为 2 (log₂), 熵的单位是 **比特** `bit`这是信息论和计算机科       学中最常用的
    - 如果底为 e (自然对数 ln), 熵的单位是 **奈特** `nat`
    - 如果底为 10 (log₁₀), 熵的单位是 **哈特** `hartley`
```
- P(x) 是事件x在真实分布中的概率
- Q(x) 是事件x在模型分布中的概率
- log₂ 表示我们用“比特”来衡量信息量

**公式解读:**
- P(x) / Q(x) : 这个比率衡量了模型Q在事件x上的预测有多“离谱”。
    - 如果 Q(x) 低估了 P(x) (即真实发生概率高，但模型预测概率低)，这个比值就很大，表示“意外”很大。
    - 如果 Q(x) 高估了 P(x), 比值就小于1。
    - 如果预测完美 P(x) = Q(x), 比值为1, log(1)=0, 没有意外, 不产生散度。
- P(x) 作为权重: 我们更关心那些真实世界中经常发生的事件 (P(x)高) 的预测准确性。如果模型在一个很罕见的事件上预测错了，影响不大；但在一个常见事件上预测错了，代价就很高。
## `Cross-entropy (CE)`
个人理解: 计算预测结果与真实结果之间的差距, CE越大预测结果越离谱

$$
H(p, q) = - \sum_{x} p(x) \cdot \log_2(q(x))
$$

**真实标签 (The Truth: p)**
对于一张确认认为猫的图片，它的“真实”概率分布 p 是非常确定的。我们通常用 one-hot 编码 来表示:
```
p = [是猫, 不是猫] = [1, 0]
```
这代表：这张图片100%是猫，0%是狗。 这个 p 是我们的基准和事实。

**模型的预测 (The Prediction: q)**
模型在看到这张图片后，会输出一个它自己预测的概率分布 q。我们来看三种情况：
- 情况A (预测得很好): 模型非常有信心地认为这是猫。
    - q1 = [0.9, 0.1] (90%可能是猫, 10%可能是狗)
- 情况B (预测得很差): 模型非常有信心地认为这是狗。
    - q2 = [0.1, 0.9] (10%可能是猫, 90%可能是狗)
- 情况C (不确定): 模型完全不确定，觉得两种可能性差不多。
    - q3 = [0.5, 0.5] (50%可能是猫, 50%可能是狗)

**3. 交叉熵的公式**
交叉熵的公式 H(p, q) 如下:
$$
H(p, q) = - \sum_{x} p(x) \cdot \log(q(x))
$$

- p(x) 是真实分布中事件x的概率。
- q(x) 是模型预测分布中事件x的概率。

**特别注意:** 因为真实标签 p 是one-hot编码, p(x) 的值要么是1，要么是0。这会极大地简化计算! 只有真实标签为1的那一项会保留下来。
```
H(p, q) = - [ p(猫)*log(q(猫)) + p(狗)*log(q(狗)) ] 
H(p, q) = - [ 1 * log(q(猫)) + 0 * log(q(狗)) ] H(p, q) = -log(q(猫))
```


所以，对于这张猫的图片，交叉熵损失就简化成了 模型预测它是猫的概率的负对数。
### `Entropy of the Distribution`
个人理解: 衡量的是一个随机事件或一个概率分布的**不确定性**或**混乱程度**, 也就是要搞清楚一个随机事件的最终结果，平均需要多少信息量（通常用比特`bit`来衡量）
- **熵越高**，代表系统越混乱，结果越不可预测
- **熵越低**，代表系统越有序，结果越容易预测

**H(X) = - Σ p(x) * log₂(p(x))**
==![[Pasted image 20250728164017.png]]

![[Pasted image 20250727173520.png]]

==Attention:==
1. CrossEntropy(p, q) = Entropy(p) + KL_Divergence(p || q) **在机器学习中, 真实数据分布 `p` 是固定的, 所以 `Entropy(p)` 是一个常数, 所以最小化交叉熵就等价于最小化KL散度**
2. CE越接近0, 则说明该事件越接近真实概率
# Regression

# `L1 Norm Loss / Mean Absolute Error Loss (MAE)`

$$ L = \frac{1}{n} \sum_{i=1}^{n} |y_i - p_i|$$
`y_i` 是真实值，`p_i` 是模型预测值，`n` 是样本数量
# `L2 Norm Loss / Mean Squared Error Loss(MSE)`

$$ L = \frac{1}{n} \sum_{i=1}^{n} |y_i - p_i|^2$$
`y_i` 是真实值，`p_i` 是模型预测值，`n` 是样本数量



