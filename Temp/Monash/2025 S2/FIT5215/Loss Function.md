---
date: 2025-08-07
tags:
  - FIT5215
author:
  - Siyuan Liu
aliases:
  - base
---
# Classification
## `Kullback-Leibler (KL) divergence`
个人理解: KL散度用来计算, 当前模型距离真实案例的偏差值

**D_KL(P || Q) = Σ P(x) * log₂( P(x) / Q(x) )**
==![[Pasted image 20250728164017.png]]==

![[Pasted image 20250727172836.png]]
# Regression
## `Mean Squared Error Loss`

# `L1 Norm Loss`
# `L2 Norm Loss`



# `Entropy of the Distribution`
个人理解: 衡量的是一个随机事件或一个概率分布的**不确定性**或**混乱程度**, 也就是要搞清楚一个随机事件的最终结果，平均需要多少信息量（通常用比特`bit`来衡量）
- **熵越高**，代表系统越混乱，结果越不可预测
- **熵越低**，代表系统越有序，结果越容易预测

**H(X) = - Σ p(x) * log₂(p(x))**
==![[Pasted image 20250728164017.png]]

![[Pasted image 20250727173520.png]]
# `Cross-entropy (CE)`
个人理解: 计算预测结果与真实结果之间的差距, CE越大预测结果越离谱

**H(p, q) = - Σ p(x) * log₂(q(x))**
==![[Pasted image 20250728164017.png]]

![[Pasted image 20250727174454.png]]
==Attention:==
1. CrossEntropy(p, q) = Entropy(p) + KL_Divergence(p || q) **在机器学习中, 真实数据分布 `p` 是固定的, 所以 `Entropy(p)` 是一个常数, 所以最小化交叉熵就等价于最小化KL散度**
2. CE越接近0, 则说明该事件越接近真实概率

# `Softmax`

**作用**: `Softmax`的作用就像一个“转换器”，它能把这些原始分数(**Logits** / **Scores**)，转换成每个选项最终的“机率”(1. 每个选项的机率都在 0% 到 100% 之间 2.所有选项的机率加起来正好是 100%)

## 计算
评委打出的原始分数如下：
- 选手A (猫): **3.2** 分
- 选手B (狗): **1.3** 分
- 选手C (鸟): **0.9** 分

**第1步：取指数 (Exponentiation)**

为了让所有分数都变成正数，并且拉开差距（让高分的优势更明显），我们对每个分数都取自然常数 `e` 的指数。

- **猫**：`e^3.2 ≈ 24.53`
- **狗**：`e^1.3 ≈ 3.67`
- **鸟**：`e^0.9 ≈ 2.46`

现在，分数都变成了正数，而且猫的优势被放大了。

**第2步：归一化 (Normalization)**

把上面得到的所有新分数加起来，得到一个总和。然后，用每个选手的新分数除以这个总和，就得到了每个选手的“得票率”。

- **总和** = `24.53 + 3.67 + 2.46 = 30.66`

现在计算每个类别的最终概率：

- **P(猫)** = `24.53 / 30.66 ≈ 0.80` (80% 的概率是猫)
- **P(狗)** = `3.67 / 30.66 ≈ 0.12` (12% 的概率是狗)
- **P(鸟)** = `2.46 / 30.66 ≈ 0.08` (8% 的概率是鸟)

**计算完成！**

我们现在得到了一组清晰的概率分布：`[0.80, 0.12, 0.08]`。

- 它们都在 0 和 1 之间。
- 它们的总和是 `0.80 + 0.12 + 0.08 = 1.0`。

现在我们可以非常自信地说：**模型认为这张图片有 80% 的可能性是猫。**