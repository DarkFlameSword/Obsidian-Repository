---
date: 2025-08-22
author:
  - Siyuan Liu
tags:
  - FIT5215
aliases:
  - base
---
# Formula
$$θt+1​=θt​−η∇θ​J(θt​)$$
- $θt​$：第 t 次迭代时的参数（模型的权重和偏置）
- $\theta_{t+1}$：更新后的参数，也就是下一次迭代用的参数
- $\eta$：学习率（learning rate），一个超参数，控制每次更新的步长大小
- J$(\theta_t)$：损失函数（loss function），衡量当前参数下模型预测与真实标签的差距。
- $\nabla_{\theta} J(\theta_t)$：损失函数对参数的梯度（gradient），告诉我们“在当前位置，往哪个方向能最快增加损失”
## 理解
可以把损失函数$J(\theta)$想象成一座山：
- 你站在某个位置 $\theta_i​$
- 梯度告诉你“往哪个方向坡度最大”
- 为了下山（最小化损失），你要往反方向走
- 每走一步就是更新一次参数，直到找到谷底（最优解）

假设损失函数是简单的二次函数：
$J(θ)=\theta^2$
那么：
$∇θ​J(θ)=2θ$
更新公式：
$θt+1​=θt​−η(2θt​)=θt​(1−2η)$
如果 $\eta$ 选得合适（比如 0.1），那么 $\theta$ 会逐渐接近 0（最优解）
# 如何计算下一个优化后的gradient
![[Pasted image 20250822171531.png]]
# 深度神经网络的训练目标
## 总体目标

$$min⁡_θ L(D;\theta)   =  \frac{1}{N}∑_{i=1}^N \ell(xi,yi;\theta)$$

- $\theta$：模型参数（权重、偏置等）
- $D=\{(xi,yi)\}_{i=1}^N$ ：数据集，有 N 个样本
- $\ell(x_i, y_i; \theta)$：对单个样本的损失
- **目标**：找到能使平均损失最小的参数 $\theta$

## ## 单样本的损失函数

$\ell(x_i, y_i; \theta) \;=\; - \log \, p(y=y_i \mid x_i)$

这就是 **交叉熵损失 (cross-entropy loss)**，它衡量模型预测的概率分布与真实标签之间的差距
## ## 分类任务里的概率模型

$p(y=y_i \mid x_i) \;=\; \frac{\exp\{ h^{(L)}_{y_i}(x_i) \}}{\sum_{m=1}^M \exp\{ h^{(L)}_m(x_i) \}}$

- $hm(L)(xi)h^{(L)}_m(x_i)hm(L)​(xi​)$：神经网络最后一层（第 LLL 层）的输出，即 **logits**，对应第 mmm 个类别。
    
- **Softmax** 函数：把 logits 转换成概率分布（所有类别概率之和为 1）