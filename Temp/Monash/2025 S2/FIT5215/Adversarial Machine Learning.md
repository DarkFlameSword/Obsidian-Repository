---
date: 2025-09-01
author:
  - Siyuan Liu
tags:
  - FIT5215
---
**ç†è§£ï¼š**
é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¾®å°æ‰°åŠ¨æ¥æ¬ºéª—æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•é˜²å¾¡è¿™äº›æ”»å‡»çš„é¢†åŸŸã€‚è¿™äº›æ‰°åŠ¨å¯¹äººç±»æ¥è¯´å‡ ä¹ä¸å¯è§ï¼Œä½†å¯ä»¥å¯¼è‡´æ¨¡å‹åšå‡ºå®Œå…¨é”™è¯¯çš„é¢„æµ‹
```
# ç›®æ ‡ï¼šæœ€å¤§åŒ–æ¨¡å‹çš„æŸå¤±
# æ–¹æ³•ï¼šæ²¿ç€æ¢¯åº¦æ–¹å‘ä¿®æ”¹è¾“å…¥

# æ­£å¸¸è®­ç»ƒï¼šè°ƒæ•´å‚æ•°Î¸æ¥å‡å°æŸå¤±
Î¸_new = Î¸ - Î± Ã— âˆ‡_Î¸ L

# å¯¹æŠ—æ”»å‡»ï¼šè°ƒæ•´è¾“å…¥xæ¥å¢å¤§æŸå¤±
x_adv = x + Îµ Ã— sign(âˆ‡_x L)
```



# æ”»å‡»ç±»å‹
## ç™½ç›’æ”»å‡»ï¼ˆWhite-box Attackï¼‰
```
# æ”»å‡»è€…å®Œå…¨äº†è§£æ¨¡å‹ï¼š
# - æ¨¡å‹æ¶æ„
# - æ¨¡å‹å‚æ•°
# - è®­ç»ƒæ•°æ®

# å¯ä»¥ç›´æ¥è®¡ç®—æ¢¯åº¦
gradients = model.backward(x, y)
perturbation = epsilon * sign(gradients)
x_adv = x + perturbation
```

---

## é»‘ç›’æ”»å‡»ï¼ˆBlack-box Attackï¼‰
```
# æ”»å‡»è€…åªèƒ½æŸ¥è¯¢æ¨¡å‹ï¼š
# - ä¸çŸ¥é“æ¨¡å‹ç»“æ„
# - ä¸çŸ¥é“å‚æ•°
# - åªèƒ½çœ‹åˆ°è¾“å‡º

# æ–¹æ³•1ï¼šè®­ç»ƒæ›¿ä»£æ¨¡å‹
substitute_model = train_similar_model()
# åœ¨æ›¿ä»£æ¨¡å‹ä¸Šç”Ÿæˆå¯¹æŠ—æ ·æœ¬
x_adv = generate_adversarial(substitute_model, x)
# å¸Œæœ›èƒ½è¿ç§»åˆ°ç›®æ ‡æ¨¡å‹

# æ–¹æ³•2ï¼šåŸºäºæŸ¥è¯¢çš„æ”»å‡»
# é€šè¿‡å¤§é‡æŸ¥è¯¢ä¼°è®¡æ¢¯åº¦
```

---
## ç›®æ ‡æ”»å‡» 
```
# ç›®æ ‡æ”»å‡»ï¼ˆTargetedï¼‰ï¼š
# è®©æ¨¡å‹é¢„æµ‹ä¸ºç‰¹å®šçš„é”™è¯¯ç±»åˆ«
# ä¾‹å¦‚ï¼šæŠŠ"ç†ŠçŒ«"é¢„æµ‹ä¸º"é•¿è‡‚çŒ¿"
# ç›®æ ‡ï¼šmin L(model(x_adv), y_target)
```

---
## éç›®æ ‡æ”»å‡»
![[Pasted image 20251001101301.png]]
```
# éç›®æ ‡æ”»å‡»ï¼ˆUntargetedï¼‰ï¼š
# åªè¦è®©æ¨¡å‹é¢„æµ‹é”™è¯¯å³å¯
# ç›®æ ‡ï¼šmax L(model(x_adv), y_true)
```


---
# ç»å…¸æ”»å‡»æ¨¡å‹
## FGSM (Fast Gradient Sign Method)
**ç†è§£ï¼š**
```
åŸå§‹å›¾åƒ x
    â†“ å‰å‘ä¼ æ’­
æŸå¤± L(x, y)
    â†“ åå‘ä¼ æ’­
æ¢¯åº¦ âˆ‡_x L
    â†“ å–ç¬¦å·
sign(âˆ‡_x L) âˆˆ {-1, 0, +1}
    â†“ ç¼©æ”¾
Îµ Ã— sign(âˆ‡_x L)
    â†“ æ·»åŠ åˆ°åŸå›¾
x_adv = x + Îµ Ã— sign(âˆ‡_x L)
```

**è®¡ç®—è¿‡ç¨‹ï¼š**
$$
x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x L(f(x;\;\theta), y))
$$
$$sign(t) = \begin{cases}
1 &\text{if t}\gt 0\\  
-1 &\text{if t}\lt 0\\
0 &\text{otherwise}\\
\end{cases}$$
- $x:$ åŸå§‹è¾“å…¥
- $x_{adv}:$ å¯¹æŠ—æ ·æœ¬
- $Îµ:$ æ‰°åŠ¨å¼ºåº¦ï¼ˆé€šå¸¸å¾ˆå°ï¼Œå¦‚0.01ï¼‰
- $L:$ æŸå¤±å‡½æ•°
- $âˆ‡_x L:$ æŸå¤±å‡½æ•°å…³äºè¾“å…¥xçš„æ¢¯åº¦
- $sign():$ ç¬¦å·å‡½æ•°ï¼ˆåªä¿ç•™æ–¹å‘ï¼‰
- $Î¸:$ æ¨¡å‹å‚æ•°
- $y:$ çœŸå®æ ‡ç­¾

---
## PGD (Projected Gradient Descent)
**ç†è§£ï¼š**
![[Pasted image 20251001104131.png]]

**è®¡ç®—è¿‡ç¨‹ï¼š**
$$\begin{aligned}
& x_0 = x+ Uniform([-\epsilon,\epsilon])\\
& \tilde{x}_{t+1} = x_t + \eta \nabla_x L(f(x;\;\theta))\\
& x_{t+1} = Proj_{B_{\epsilon}(x)}(\tilde{x}_{t+1})\\
& ğ‘¥_{ğ‘ğ‘‘ğ‘£}= ğ‘¥_k
\end{aligned}$$
- $k$: Run in ğ‘˜ steps (ğ‘˜ = 20)
- $ğœ‚ > 0$: is the learning rate