---
date: 2025-08-24
author:
  - Siyuan Liu
tags:
  - FIT5215
aliases:
  - summary
---
![[Pasted image 20250824192446.png]]
# Overview of Optimization problem in ML and DL
![[Pasted image 20250908142412.png]]

---
# weight initialization
## å¼•è¨€
**What is a good weight/filter initialization?**

- Break the â€˜symmetryâ€™ of the network: two hidden nodes with the same input should have different weights
    - Large initial weights has better symmetry breaking effect, help avoiding losing signals and redundant units, but could result in exploding values during back-ward and forward passes, especially in Recurrent Neural Networks
- the gradient will not vanishing or exploding
- avoid overfitting

**æƒé‡åˆå§‹åŒ–å¯¹è±¡**

- å…¨è¿æ¥å±‚çš„æƒé‡
- å·ç§¯å±‚çš„æƒé‡
- ä»»ä½•éœ€è¦å­¦ä¹ çš„çº¿æ€§å˜æ¢çš„æƒé‡

**ä½œç”¨**
- åŠ å¿«æ¨¡å‹æ”¶æ•›ï¼ˆè‰¯å¥½çš„åˆå§‹åŒ–è®©ç½‘ç»œä»ä¸€ä¸ª"åˆç†"çš„èµ·ç‚¹å¼€å§‹ï¼Œè€Œä¸æ˜¯éšæœºæ¼«æ­¥ï¼‰
- é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- æ‰“ç ´å¯¹ç§°æ€§ï¼ˆSymmetry Breakingï¼‰
- ä¿æŒè¾“å…¥è¾“å‡ºæ–¹å·®ç¨³å®š

---
## Xavier Weight Initialization
**ä½œç”¨:**
Try to ensure the variance of the outputs of each layer equal to the variance of its input. This way, signals and gradients don't shrink or amplify layer by layer in the network

**è®¡ç®—æ­¥éª¤:**
å‡è®¾æŸä¸€å±‚æœ‰ï¼š
- è¾“å…¥å•å…ƒæ•°ï¼š$n_{in}$
- è¾“å‡ºå•å…ƒæ•°ï¼š$n_{out}$
æƒé‡çŸ©é˜µ W çš„å…ƒç´ å¸Œæœ›æ»¡è¶³ï¼š
$$Var(W x) \approx Var(x)$$

Xavier åˆå§‹åŒ–ç»™å‡ºäº†ä¸€ä¸ªç®€å•å…¬å¼ï¼š
- å¯¹**å‡åŒ€åˆ†å¸ƒ**ï¼š
$$W \sim U\left[
-\sqrt{\frac{6}{n_{in} + n_{out}}},\sqrt{\frac{6}{n_{in} + n_{out}}}
\right]$$

- å¯¹**æ­£æ€åˆ†å¸ƒ**ï¼š
$$W \sim N\left(0, \frac{2}{n_{in} + n_{out}}\right)$$

- â€‹ $n_{in}$æ˜¯è¾“å…¥èŠ‚ç‚¹æ•°
- $n_{out}$æ˜¯è¾“å‡ºèŠ‚ç‚¹æ•°
**é€‚åº”åœºæ™¯:**
- `sigmoid`, `tanh`
    - å› ä¸ºè¿™ä¸¤ä¸ªå‡½æ•°åœ¨è¾“å…¥è¾ƒå¤§æ—¶ä¼šé¥±å’Œï¼Œå®¹æ˜“å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
- ä¸é€‚ç”¨`ReLU`

---

## He Weight Initialization
**ä½œç”¨:**
Ensure the variance of the outputs of each layer equal to the variance of its inputs, but `He` only adapt `ReLU` or similar

**Why?:**
Xavier åˆå§‹åŒ–å‡è®¾æ¿€æ´»å‡½æ•°è¿‘ä¼¼**çº¿æ€§**ï¼Œä½† ReLU å¹¶éå¯¹ç§°çº¿æ€§å‡½æ•°ï¼Œç‰¹åˆ«æ˜¯å®ƒä¼šæŠŠè´Ÿæ•°å…¨éƒ¨ç½®é›¶ï¼Œè¿™ä¼šæ”¹å˜è¾“å‡ºçš„æ–¹å·®ã€‚å› æ­¤ï¼Œéœ€è¦é’ˆå¯¹ ReLU è®¾è®¡æ–°çš„åˆå§‹åŒ–æ–¹å¼

**è®¡ç®—æ­¥éª¤:**
å‡è®¾æŸä¸€å±‚æœ‰ï¼š
- è¾“å…¥å•å…ƒæ•°ï¼š$n_{in}$
- è¾“å‡ºå•å…ƒæ•°ï¼š$n_{out}$
`ReLU` çš„ç‰¹ç‚¹æ˜¯ï¼š
$$\text{ReLU}(x) = \max(0,x)$$

å¤§çº¦ **ä¸€åŠçš„è¾“å…¥ä¼šè¢«ç½®ä¸º 0**ï¼Œå› æ­¤è¾“å‡ºçš„æ–¹å·®ä¼šå‡åŠã€‚ä¸ºäº†ä¿è¯è¾“å‡ºçš„æ–¹å·®å’Œè¾“å…¥ç›¸åŒï¼Œæˆ‘ä»¬éœ€è¦åœ¨åˆå§‹åŒ–æ—¶æŠŠæ–¹å·®æ”¾å¤§ä¸€ç‚¹ï¼š
- å¯¹**å‡åŒ€åˆ†å¸ƒ**ï¼š
$$W \sim U\left[
-\sqrt{\frac{6}{n_{in}}},\sqrt{\frac{6}{n_{in}}}
\right]$$

- å¯¹**æ­£æ€åˆ†å¸ƒ**ï¼š
$$W \sim N\left(0, \alpha \times \sqrt{\frac{2}{n_{in}+n_{out}}}\right)\; \alpha = \begin{cases} 1 & \text{if sigmoid} \\ 4 & \text{if tanh} \\ \sqrt{2} & \text{if ReLU} \end{cases} $$

- â€‹ $n_{in}$æ˜¯è¾“å…¥èŠ‚ç‚¹æ•°
- $n_{out}$æ˜¯è¾“å‡ºèŠ‚ç‚¹æ•°
- $\alpha = \sqrt2$ï¼Œå› ä¸º `ReLU` ä¼šä¸¢æ‰ä¸€åŠçš„ä¿¡å·

**é€‚åº”åœºæ™¯:**
- `ReLU`, `ReLUçš„æ‰€æœ‰å˜ç§`
- æ·±å±‚å·ç§¯ç¥ç»ç½‘ç»œ / å‰é¦ˆç½‘ç»œ éƒ½å¯ä»¥ç”¨ He åˆå§‹åŒ–

---

# Regularization Techniques
## Regularization related to Weight
### L1 / L2 Regularization
æŸå¤±å‡½æ•°åŸæœ¬æ˜¯ï¼š

$$J(\theta) = \frac{1}{N}\sum_{i=1}^N l\big(f(x_i;\theta), y_i\big)$$
- $J(\theta):$ æˆæœ¬å‡½æ•°

#### L1 Regularization

$$J(\theta) = \frac{1}{N}\sum_{i=1}^N l\big(f(x_i;\theta), y_i\big) + \frac{\lambda}{N} R(\theta)$$

- $\lambda > 0$ï¼šregularization parameter (æ­£åˆ™åŒ–å¼ºåº¦ç³»æ•°)
- $R(\theta)$ï¼šæ­£åˆ™åŒ–é¡¹
$$R(Î¸)=||\theta||=\sum_jâˆ£Î¸_jâ€‹âˆ£$$
- $Î¸:$ æ˜¯æ¨¡å‹çš„å‚æ•°å‘é‡ï¼Œä¾‹å¦‚ $Î¸ = [Î¸â‚, Î¸â‚‚, Î¸â‚ƒ, ..., Î¸â‚™]$
- $j:$ æ˜¯ç´¢å¼•ï¼Œä» 1 åˆ° nï¼ˆæˆ–ä» 0 åˆ° n-1ï¼Œå–å†³äºç´¢å¼•çº¦å®šï¼‰
- $Î¸â±¼:$ è¡¨ç¤ºå‚æ•°å‘é‡ä¸­ç¬¬ j ä¸ªå‚æ•°

**ç‰¹ç‚¹**ï¼šé¼“åŠ±å‚æ•°å˜ä¸º 0ï¼Œå¾—åˆ°**ç¨€ç–æ¨¡å‹**ï¼ˆå¾ˆå¤šæƒé‡ä¸º 0ï¼‰
**ç”¨é€”**ï¼šç‰¹å¾é€‰æ‹©ï¼ˆè‡ªåŠ¨æŠŠä¸é‡è¦çš„ç‰¹å¾æƒé‡å‹åˆ° 0ï¼‰,å‹ç¼©æ¨¡å‹

---
#### L2 Regularization
$$J(\theta) = \frac{1}{N}\sum_{i=1}^N l\big(f(x_i;\theta), y_i\big) + \frac{\lambda}{2N} R(\theta)$$

- $\lambda > 0$ï¼šregularization parameter (æ­£åˆ™åŒ–å¼ºåº¦ç³»æ•°)
- $R(\theta)$ï¼šæ­£åˆ™åŒ–é¡¹
$$R(Î¸)=||\theta||^2â€‹=\sum_j \sqrt{Î¸_j^2}â€‹$$
- $Î¸:$ æ˜¯æ¨¡å‹çš„å‚æ•°å‘é‡ï¼Œä¾‹å¦‚ $Î¸ = [Î¸â‚, Î¸â‚‚, Î¸â‚ƒ, ..., Î¸â‚™]$
- $j:$ æ˜¯ç´¢å¼•ï¼Œä» 1 åˆ° nï¼ˆæˆ–ä» 0 åˆ° n-1ï¼Œå–å†³äºç´¢å¼•çº¦å®šï¼‰
- $Î¸â±¼:$ è¡¨ç¤ºå‚æ•°å‘é‡ä¸­ç¬¬ j ä¸ªå‚æ•°

**ç‰¹ç‚¹**ï¼šæƒ©ç½šå¤§æƒé‡ï¼Œé¼“åŠ±å‚æ•°æ›´å‡åŒ€åˆ†å¸ƒï¼Œä¸ä¼šç›´æ¥å˜æˆ 0
**ç”¨é€”**ï¼šå¸¸ç”¨æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œlossé«˜æ–¹å·®

---
## Regularization related to Construction
### Dropout
![[Pasted image 20250824212935.png]]
**ç†è§£:**
In each iteration, at each layer, randomly choose some neurons and drop all connections from these neurons

**ä½œç”¨**:
- é˜²æ­¢è¿‡æ‹Ÿåˆ
- å‡å°‘è®¡ç®—é‡

**ä»£ç :**
```
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(128, 64),
    nn.ReLU(),
    # å¸¸ç”¨ 0.2â€“0.5
    # è¾“å…¥å±‚ä¸€èˆ¬ç”¨å°ä¸€äº›,å¦‚ 0.1â€“0.2
    # éšè—å±‚å¸¸ç”¨ 0.5 å·¦å³
    nn.Dropout(p=0.5),  
    
- éšè—å±‚å¸¸ç”¨ 0.5 å·¦å³
    nn.Linear(64, 10)
)

```

---
## Regularization related to Data
### Data Augmentation
**ç†è§£:**
åœ¨ä¸é¢å¤–æ”¶é›†æ–°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¯¹å·²æœ‰æ•°æ®è¿›è¡Œå˜æ¢ï¼Œæ¥äººå·¥å¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§çš„ä¸€ç§æ–¹æ³•

**ä¾‹å­:**
- å›¾åƒæ•°æ®

	- **å‡ ä½•å˜æ¢**
    
	    - ç¿»è½¬ï¼ˆhorizontal / vertical flipï¼‰:
			 **ç¦ç”¨:** 
				å½“æ°´å¹³æ–¹å‘å…·æœ‰ç‰¹å®šå«ä¹‰æ—¶ã€‚ä¾‹å¦‚ï¼Œæ–‡å­—è¯†åˆ« (OCR)ï¼Œå­—æ¯ "b" ç¿»è½¬åä¼šå˜æˆ "d"ï¼Œå«ä¹‰å®Œå…¨æ”¹å˜ã€‚éœ€è¦åŒºåˆ†å·¦å³çš„ä»»åŠ¡ï¼Œæ¯”å¦‚è¯†åˆ«å·¦æ‰‹å’Œå³æ‰‹
				ç»å¤§å¤šæ•°çš„è‡ªç„¶åœºæ™¯ã€‚å› ä¸ºé‡åŠ›çš„å­˜åœ¨ï¼Œæˆ‘ä»¬å¾ˆå°‘çœ‹åˆ°ä¸Šä¸‹é¢ å€’çš„æ±½è½¦ã€äººæˆ–æ ‘æœ¨ã€‚å¯¹è¿™äº›å›¾åƒè¿›è¡Œå‚ç›´ç¿»è½¬ä¼šç”Ÿæˆä¸çœŸå®çš„æ ·æœ¬ï¼Œå¯èƒ½ä¼šè¯¯å¯¼æ¨¡å‹
		        
	    - æ—‹è½¬ï¼ˆrotationï¼‰
			 **ç¦ç”¨:** 
			 åœ¨éœ€è¦ä¸¥æ ¼å¯¹é½çš„ä»»åŠ¡ä¸­ï¼Œè¿‡åº¦æ—‹è½¬å¯èƒ½ä¼šå¸¦æ¥é—®é¢˜
			 æ³¨æ„æ—‹è½¬åäº§ç”Ÿçš„é»‘è‰²è¾¹è§’ï¼Œéœ€è¦ç”¨æŸç§æ–¹å¼å¡«å……ï¼ˆå¦‚åå°„å¡«å……ã€å¸¸æ•°å¡«å……ç­‰ï¼‰ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥ä¸å¿…è¦çš„å™ªå£°
			 
	    - å¹³ç§»ï¼ˆtranslationï¼‰
			 **ç¦ç”¨:** 
			 æ— (æ³¨æ„æ—‹è½¬åäº§ç”Ÿçš„é»‘è‰²è¾¹è§’ï¼Œéœ€è¦ç”¨æŸç§æ–¹å¼å¡«å……ï¼ˆå¦‚åå°„å¡«å……ã€å¸¸æ•°å¡«å……ç­‰ï¼‰ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥ä¸å¿…è¦çš„å™ªå£°)
			 
	    - ç¼©æ”¾ï¼ˆscaling, zoomï¼‰
			 **ç¦ç”¨:** 
			 å½“å›¾åƒä¸­æœ‰éå¸¸å°çš„ç‰©ä½“æ—¶ï¼Œè¿‡åº¦ç¼©å°å¯èƒ½ä¼šå¯¼è‡´ç‰©ä½“ä¿¡æ¯å®Œå…¨ä¸¢å¤±
			 
	    - å‰ªè£ï¼ˆrandom crop, center cropï¼‰
	         **ç¦ç”¨:** 
			 å¦‚æœéšæœºå‰ªè£çš„åŒºåŸŸå¤ªå°ï¼Œå¯èƒ½ä¼šè£æ‰ç‰©ä½“çš„ä¸»ä½“éƒ¨åˆ†ï¼Œå¯¼è‡´å‰ªè£å‡ºçš„å›¾åƒå—ä¸å†åŒ…å«æœ‰æ•ˆçš„ç‰©ä½“ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¯¹ä¸€å¼ ç‹—çš„å›¾ç‰‡ï¼Œéšæœºå‰ªè£å¯èƒ½åªè£åˆ°äº†ä¸€å—è‰åœ°
		- æ‰­æ›²ï¼ˆdistortionï¼‰

	- **é¢œè‰²ä¸å…‰ç…§è°ƒæ•´**
	    
	    - éšæœºäº®åº¦ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦å˜åŒ–
	        
	    - è‰²å½©æŠ–åŠ¨ï¼ˆcolor jitterï¼‰
	        
	    - ç°åº¦åŒ–
	        
	- **å™ªå£°ä¸æ¨¡ç³Š**
	    
	    - åŠ å…¥é«˜æ–¯å™ªå£°
	        
	    - æ¨¡ç³Šå¤„ç†ï¼ˆGaussian blur, motion blurï¼‰
	        
	- **é«˜çº§æ–¹æ³•**
	    
	    - **Cutout**ï¼šéšæœºé®æŒ¡éƒ¨åˆ†åŒºåŸŸ
	        
	    - **Mixup**ï¼šå°†ä¸¤å¼ å›¾åƒæ··åˆ
	        
	    - **CutMix**ï¼šå°†ä¸€å¼ å›¾åƒçš„éƒ¨åˆ†åŒºåŸŸæ›¿æ¢ä¸ºå¦ä¸€å¼ 
        

- æ–‡æœ¬æ•°æ®

	- åŒä¹‰è¯æ›¿æ¢ï¼ˆsynonym replacementï¼‰
	    
	- éšæœºæ’å…¥ / åˆ é™¤ / äº¤æ¢è¯è¯­
	    
	- å›è¯‘ï¼ˆback translationï¼Œä¾‹å¦‚ä¸­è¯‘è‹±å†è¯‘å›ä¸­ï¼‰
	    
	- ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆå¢å¼ºå¥å­ï¼ˆå¦‚ GPT ç”Ÿæˆ paraphraseï¼‰
    
- éŸ³é¢‘æ•°æ®

	- æ—¶é—´æ‹‰ä¼¸ï¼ˆtime stretchingï¼‰
	    
	- éŸ³è°ƒå˜åŒ–ï¼ˆpitch shiftingï¼‰
	    
	- åŠ å™ªå£°ï¼ˆbackground noiseï¼‰
	    
	- è£å‰ªã€æ‹¼æ¥
    
- è¡¨æ ¼æ•°æ®

	- å¯¹æ•°å€¼ç‰¹å¾åŠ å™ªå£°
	    
	- SMOTEï¼ˆåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯ï¼Œç”¨äºç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼‰
**ä»£ç :**
```
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomRotation(15),      # éšæœºæ—‹è½¬ Â±15Â°
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor()
])

```

---
### Label smoothing
### Data mix-up
### Data Cut-mix
## Indirect Regularization
### Batch Normalization
**ç†è§£:**
ç½‘ç»œæ¯ä¸€å±‚çš„è¾“å…¥åˆ†å¸ƒåœ¨è®­ç»ƒä¸­ä¸æ–­æ”¹å˜ â†’ å¯¼è‡´è®­ç»ƒéš¾ä»¥æ”¶æ•›ï¼›æ‰€ä»¥é€šè¿‡BN ç­‰æ–¹æ³•è§„èŒƒåŒ–è¾“å…¥æ¥ç¼“è§£`Internal Covariate Shift`

**å…¬å¼:**
Let $ğ‘§ = ğ‘Š^kâ„^k + ğ‘^ğ‘˜$ be the mini-batch before activation

åœ¨æ¯ä¸€å±‚å¯¹è¾“å…¥åšæ ‡å‡†åŒ–ï¼š
$$\mu = \frac{1}{m} \sum_{i=1}^mz_i$$
- $m:$ mini-batchæœ‰mä¸ªæ•°æ®

$$\sigma^2 = \frac{1}{m} \sum_{i=1}^m(z_i - \mu)$$
- $m:$ mini-batchæœ‰mä¸ªæ•°æ®


$$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2+\epsilon}}$$
- $\mu$: mini-batch çš„å‡å€¼
- $\sigma^2$: mini-batch çš„æ–¹å·®
- $\epsilon$: a small value such as $1e^{-7}$
- $x$: mini-batch çš„è¾“å…¥h
- $\hat x$: mini-batch ç¬¬ä¸€æ¬¡å¤„ç†åçš„è¾“å…¥
$$y = \gamma \hat{x} + \beta$$
- $\gamma$: å¯è®­ç»ƒå‚æ•°,ä¿è¯ç½‘ç»œæœ‰è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›
- $\beta$: å¯è®­ç»ƒå‚æ•°,ä¿è¯ç½‘ç»œæœ‰è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›
- $y$: mini-batchåœ¨BNåçš„è¾“å…¥

**æ³¨æ„:**
- In training phase, it uses the batch statistics**æ‰¹æ¬¡ç»Ÿè®¡é‡** (mean and std)
- In testing phase, it uses the population statistics**æ€»ä½“ç»Ÿè®¡é‡** (mean and std)
- always used in deep layers(more than 10)

**ä½œç”¨:**
- Cope with internal covariate shift
- Reduce gradient
- vanishing/exploding
- Reduce overfitting
- Make training more stable
- Converge faster

---


## Early Stopping
![[Pasted image 20250824211637.png]]
**ç†è§£:**
åœ¨éªŒè¯é›†è¡¨ç°æœ€å¥½çš„æ—¶å€™åœä¸‹æ¥

**ä»£ç :**
```
import torch

class EarlyStopping:
    def __init__(self, patience=5, delta=0.0):
        self.patience = patience # å¦‚æœè¿ç»­`patience`ä¸ªepochéªŒè¯æŸå¤±éƒ½æ²¡æœ‰æ”¹å–„ï¼Œå°±è§¦å‘æ—©åœ
        self.delta = delta # åªæœ‰å½“ `new_loss < best_loss - delta` æ—¶æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆæ”¹å–„
        self.best_loss = float('inf') # è®°å½•åˆ°ç›®å‰ä¸ºæ­¢é‡åˆ°çš„æœ€å°éªŒè¯æŸå¤±
        self.counter = 0 # è®°å½•è¿ç»­å¤šå°‘ä¸ªepochéªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„
        self.early_stop = False # å½“`counter >= patience`æ—¶è®¾ä¸ºTrue
        self.best_model_state = None

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.counter = 0
            self.best_model_state = model.state_dict()  # ä¿å­˜æœ€ä½³æƒé‡
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
early_stopping = EarlyStopping(patience=5)

for epoch in range(100):
    train(...)   # è®­ç»ƒ
    val_loss = validate(...)  # éªŒè¯
    
    early_stopping(val_loss, model)

    if early_stopping.early_stop:
        print("Early stopping!")
        model.load_state_dict(early_stopping.best_model_state)  # æ¢å¤æœ€ä½³å‚æ•°
        break
```

==ä¸ªäººå»ºè®®é¿å…ä½¿ç”¨early stoppingï¼š==
- éªŒè¯æŸå¤±å¯èƒ½å­˜åœ¨çŸ­æœŸæ³¢åŠ¨ï¼Œearly stoppingå¯èƒ½åœ¨å…¨å±€æœ€ä¼˜ç‚¹ä¹‹å‰å°±åœæ­¢
- éªŒè¯é›†ä¸æµ‹è¯•é›†åˆ†å¸ƒä¸ä¸€è‡´æ—¶æ•ˆæœä¸ä½³
- ä¸dropoutä¸€èµ·ä½¿ç”¨å¯èƒ½å¯¼è‡´éªŒè¯æŸå¤±ä¸ç¨³å®š
- ä¸èƒ½å’ŒBatch Normalizationä¸€èµ·ä½¿ç”¨ï¼Œå› ä¸ºBNå±‚åœ¨è®­ç»ƒ/è¯„ä¼°æ¨¡å¼ä¸‹è¡Œä¸ºä¸åŒ

---
# Transfer Learning
**ç†è§£:**
Remove FC layers from the pretrained model, then replace them with a brand-new FC head

**é€‚ç”¨åœºæ™¯:**
- **æ ‡æ³¨æ•°æ®ä¸è¶³**
    - è®­ç»ƒå¤§æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®ï¼Œä½†å¾ˆå¤šä»»åŠ¡æ•°æ®ç¨€ç¼ºï¼ˆä¾‹å¦‚åŒ»å­¦å›¾åƒã€ä½èµ„æºè¯­è¨€ï¼‰
- **è®­ç»ƒæˆæœ¬å¤ªé«˜**
    - ä»é›¶å¼€å§‹è®­ç»ƒæ·±åº¦ç½‘ç»œéœ€è¦å¤§é‡ç®—åŠ›
- **ç›¸ä¼¼ä»»åŠ¡é—´çŸ¥è¯†å¯è¿ç§»**
    - æ¯”å¦‚å›¾åƒç‰¹å¾ã€è¯­è¨€æ¨¡å‹ä¸­çš„è¯å‘é‡ï¼Œè¿™äº›éƒ½æ˜¯é€šç”¨çš„ï¼Œå¯ä»¥å¤ç”¨


## Feature Transfer
**ç‰¹ç‚¹:**
- ä½¿ç”¨åœ¨å¤§æ•°æ®é›†ï¼ˆå¦‚ ImageNetï¼‰ä¸Šé¢„è®­ç»ƒå¥½çš„æ¨¡å‹æå–ç‰¹å¾
- åœ¨æ–°ä»»åŠ¡ä¸­åªè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼ˆå¦‚ SVMã€å…¨è¿æ¥å±‚ï¼‰

**ä»£ç **:
```
# pip install torch torchvision scikit-learn
import torch, numpy as np
import torch.nn as nn
from torchvision import models, datasets, transforms
from torch.utils.data import DataLoader
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1) é¢„å¤„ç†ä¸æ•°æ®
tfm = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224),
                          transforms.ToTensor(), transforms.Normalize(
                          mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])
train_set = datasets.ImageFolder("data/train", tfm)
val_set   = datasets.ImageFolder("data/val",   tfm)
train_loader = DataLoader(train_set, batch_size=64, shuffle=False, num_workers=2)
val_loader   = DataLoader(val_set,   batch_size=64, shuffle=False, num_workers=2)

# 2) é¢„è®­ç»ƒæ¨¡å‹åšç‰¹å¾æå–ï¼ˆå»æ‰æœ€ååˆ†ç±»å±‚ï¼‰
backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
backbone.fc = nn.Identity()   # ç›´æ¥è¾“å‡ºå…¨å±€ç‰¹å¾
backbone.eval().cuda()

def extract_feats(loader):
    X, y = [], []
    with torch.no_grad():
        for imgs, labels in loader:
            feats = backbone(imgs.cuda()).cpu().numpy()
            X.append(feats); y.append(labels.numpy())
    return np.vstack(X), np.concatenate(y)

Xtr, ytr = extract_feats(train_loader)
Xva, yva = extract_feats(val_loader)

# 3) åªè®­ç»ƒä¸€ä¸ªè½»é‡åˆ†ç±»å™¨ï¼ˆé€»è¾‘å›å½’ / ä¹Ÿå¯ SVMï¼‰
clf = LogisticRegression(max_iter=2000, n_jobs=-1)
clf.fit(Xtr, ytr)
pred = clf.predict(Xva)
print("Val acc:", accuracy_score(yva, pred))

```
## Fine-Tuning
**æ­¥éª¤:**
![[Pasted image 20250824221050.png]]
1. Freeze all CONV layers in the network
2. Only allow the gradient to backpropagate through the FC layers. Doing this allows our network to warm up(1-5 epoch)
![[Pasted image 20250824221458.png]]
3. unfreeze all layers in the network
4. Continue training the entire network, but with a very small learning rate
5. We do not want to deviate our CONV filters dramatically. Training is then allowed to continue until sufficient accuracy is obtained

**ç‰¹ç‚¹:**
- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ã€‚
- åœ¨æ–°ä»»åŠ¡ä¸Šç»§ç»­è®­ç»ƒï¼š
    - **å†»ç»“å‰å‡ å±‚**ï¼ˆä¿æŒé€šç”¨ç‰¹å¾ï¼Œå¦‚è¾¹ç¼˜ã€é¢œè‰²ï¼‰ï¼Œåªè®­ç»ƒåå‡ å±‚
    - æˆ–è€… **å…¨æ¨¡å‹å¾®è°ƒ**ï¼Œå­¦ä¹ ç‡è®¾ç½®è¾ƒå°

**ä»£ç **:
```
import torch
import torchvision.models as models
import torch.nn as nn

# åŠ è½½é¢„è®­ç»ƒçš„ ResNet18
model = models.resnet18(pretrained=True)

# å†»ç»“å‰é¢çš„å±‚
for param in model.parameters():
    param.requires_grad = False

# æ›¿æ¢æœ€åä¸€å±‚åˆ†ç±»å™¨ï¼ˆå‡è®¾æ–°ä»»åŠ¡æœ‰ 10 ç±»ï¼‰
model.fc = nn.Linear(model.fc.in_features, 10)

# ç°åœ¨åªè®­ç»ƒæœ€åä¸€å±‚

```

# Ill-conditioning problem

# Long-term dependencies

# Poor correspondence between local and global structures

# Theoretical limits of optimization (but they usually have little use in practice of deep learning)


