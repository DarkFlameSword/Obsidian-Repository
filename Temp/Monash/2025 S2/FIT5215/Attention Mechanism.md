---
date: 2025-10-04
author:
  - Siyuan Liu
tags:
  - FIT5215
---
# 作者简笔
关于注意力机制，从Monash FIT5215 提供的PPT教案来看是脱离实际的，建议结合网络教学资源。
例如：[[https://www.youtube.com/watch?v=PSs6nxngL6k&list=PLJI03vBSNJqQQVt-6T4dHcy0V_R0p3gL7&index=26|# Attention for Neural Networks, Clearly Explained!!!]]

![[Pasted image 20251004165159.png]]

# Attention Mechanism Type
## Global attention
Use all input hidden states of the encoder when deriving the context $c_t$
## Local attention
Use a selective window of input hidden states of the encoder when deriving the context  $c_t$

## Self-attention/Multiple Heads/Transformer Networks

## Pyramidal encoders

## Hierarchical Attention

## Soft/Hard Attention